---
x-default-logging:
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.Name}}"

networks:
  net:
    driver: bridge
services:
# docker run --name pgvector-container -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16
  db:
    image: pgvector/pgvector:pg14
    container_name: pgvector
    ports:
      - 7432:5432
    volumes:
      # This script initialises the DB for integration tests
      - ./docker/pgvector/scripts:/docker-entrypoint-initdb.d
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=langchain
      - POSTGRES_USER=langchain
      - POSTGRES_DB=langchain

    restart: unless-stopped
    # logging: *logging
    networks:
      - net
    command: |
      postgres -c log_statement=all
    # healthcheck:
    #   test:
    #     [
    #       "CMD-SHELL",
    #       "psql postgresql://langchain:langchain@localhost/langchain --command 'SELECT 1;' || exit 1",
    #     ]
    #   interval: 5s
    #   retries: 60

  # postgres:
  #   image: postgres:14-alpine
  #   ports:
  #     - 5433:5432
  #   volumes:
  #     - postgres-vector-admin-data:/var/lib/postgresql/data
  #   environment:
  #     POSTGRES_USER: vectoradmin
  #     POSTGRES_PASSWORD: password
  #     POSTGRES_DB: vdbms
  #   networks:
  #     - net
  #   logging: *logging

  # zipkin:
  #   image: openzipkin/zipkin
  #   ports:
  #     - "9411:9411" # you can access Zipkin UI at http://localhost:9411
  #   # depends_on: [otel-collector]
  #   environment:
  #     - JAVA_OPTS=-Xms1024m -Xmx1024m -XX:+ExitOnOutOfMemoryError
  #   depends_on:
  #     otel-collector:
  #       condition: service_started
  #   networks:
  #     - net

  # otel-collector:
  #   image: ${COLLECTOR_CONTRIB_IMAGE}
  #   command: ["--config=/etc/otel-collector-config.yaml"]
  #   container_name: otel-collector
  #   volumes:
  #     - ${PWD}/examples/observability/otel-collector-config.yaml:/etc/otel-collector-config.yaml
  #   ports:
  #     - "4317:4317"  # OTLP gRPC receiver
  #     - "4318:4318"  # HTTP
  #     - "55681:55681" # Legacy
  #     - "1888:1888"   # pprof extension
  #     - "8888:8888"   # Prometheus metrics exposed by the collector
  #     - "8889:8889"   # Prometheus exporter metrics
  #     - "13133:13133" # health_check extension
  #     - "55679:55679" # zpages extension
  #   depends_on:
  #     - jaeger
  #   logging: *logging
  #   networks:
  #     - net

  server:
    # image: server
    image: chromadb/chroma:latest
    container_name: chroma
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    volumes:
      # Be aware that indexed data are located in "/chroma/chroma/"
      # Default configuration for persist_directory in chromadb/config.py
      # Read more about deployments: https://docs.trychroma.com/deployment
      # - chroma-data:/chroma/chroma
      - ./src/goob_ai/data/chroma/vectorstorage:/chroma/chroma:rw

    command: "--workers 1 --host 0.0.0.0 --port 8010 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive\
      \ 30"
    environment:
      - IS_PERSISTENT=TRUE
      - ALLOW_RESET=TRUE
      - CHROMA_SERVER_AUTHN_PROVIDER=${CHROMA_SERVER_AUTHN_PROVIDER}
      - CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=${CHROMA_SERVER_AUTHN_CREDENTIALS_FILE}
      - CHROMA_SERVER_AUTHN_CREDENTIALS=${CHROMA_SERVER_AUTHN_CREDENTIALS}
      - CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=${CHROMA_AUTH_TOKEN_TRANSPORT_HEADER}
      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}
      # - CHROMA_OTEL_COLLECTION_ENDPOINT=http://otel-collector:4318/
      # - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}
      # - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME:-chroma}
      # - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY:-all}
      # - OTEL_EXPORTER_OTLP_ENDPOINT
      # - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      # - OTEL_RESOURCE_ATTRIBUTES
      # - OTEL_SERVICE_NAME=chroma
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
      # - PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      # - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}
      # - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}
      # - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}
      # - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}
      - CHROMA_SERVER_NOFILE=${CHROMA_SERVER_NOFILE}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    restart: unless-stopped # possible values are: "no", always", "on-failure", "unless-stopped"

    # depends_on:
    #   otel-collector:
    #     condition: service_started
    ports:
      - "8010:8010"
    healthcheck:
      # Adjust below to match your container port
      test:
        - "CMD"
        - "curl"
        - "-f"
        - "http://localhost:8010/api/v1/heartbeat"
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - net

  # NOTE: Use http://host.docker.internal:8010 to access the server from the admin container
  chromadb-admin:
    image: "fengzhichao/chromadb-admin:latest"
    # For DHCP it is recommended to remove these ports and instead add: network_mode: "host"
    # hostname: 'chromadb-admin'
    ports:
      - "3000:3000/tcp"
    container_name: chromadb-admin
    expose:
      - 3000
    restart: unless-stopped
    networks:
      - net
    # depends_on:
    # - server
    depends_on:
      server:
        condition: service_started

  # # ********************
  # # Telemetry Components
  # # ********************
  # # Jaeger
  # jaeger:
  #   image: ${JAEGERTRACING_IMAGE}
  #   container_name: jaeger
  #   command:
  #     - "--memory.max-traces=5000"
  #     - "--query.base-path=/jaeger/ui"
  #     - "--prometheus.server-url=http://${PROMETHEUS_ADDR}"
  #     - "--prometheus.query.normalize-calls=true"
  #     - "--prometheus.query.normalize-duration=true"
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 400M
  #   restart: unless-stopped
  #   ports:
  #     # - "${JAEGER_SERVICE_PORT}"         # Jaeger UI
  #     # - "${OTEL_COLLECTOR_PORT_GRPC}"
  #     # SOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/demo/docker-compose.yaml
  #     - "16686:16686"
  #     - "14268"
  #     - "14250"
  #   environment:
  #     - METRICS_STORAGE_TYPE=prometheus
  #   logging: *logging
  #   networks:
  #     - net

  # # Grafana
  # grafana:
  #   image: ${GRAFANA_IMAGE}
  #   container_name: grafana
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 100M
  #   restart: unless-stopped
  #   environment:
  #     - "GF_INSTALL_PLUGINS=grafana-opensearch-datasource"
  #   volumes:
  #     - ./src/grafana/grafana.ini:/etc/grafana/grafana.ini
  #     - ./src/grafana/provisioning/:/etc/grafana/provisioning/
  #   ports:
  #     - "${GRAFANA_SERVICE_PORT}"
  #   logging: *logging
  #   networks:
  #     - net
  # # Valkey used by Cart service
  # valkey-cart:
  #   image: ${VALKEY_IMAGE}
  #   container_name: valkey-cart
  #   user: valkey
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 20M
  #   restart: unless-stopped
  #   ports:
  #     - "${VALKEY_PORT}"
  #   logging: *logging
  #   networks:
  #     - net
  # # # OpenTelemetry Collector
  # # otelcol:
  # #   image: ${COLLECTOR_CONTRIB_IMAGE}
  # #   container_name: otel-col
  # #   deploy:
  # #     resources:
  # #       limits:
  # #         memory: 200M
  # #   restart: unless-stopped
  # #   command: [ "--config=/etc/otelcol-config.yml", "--config=/etc/otelcol-config-extras.yml" ]
  # #   user: 0:0
  # #   volumes:
  # #     - ${DOCKER_SOCK}:/var/run/docker.sock:ro
  # #     - ${OTEL_COLLECTOR_CONFIG}:/etc/otelcol-config.yml
  # #     - ${OTEL_COLLECTOR_CONFIG_EXTRAS}:/etc/otelcol-config-extras.yml
  # #   ports:
  # #     - "${OTEL_COLLECTOR_PORT_GRPC}"
  # #     - "${OTEL_COLLECTOR_PORT_HTTP}"
  # #   depends_on:
  # #     - jaeger
  # #   logging: *logging
  # #   environment:
  # #     - ENVOY_PORT

  # # Prometheus
  # prometheus:
  #   image: ${PROMETHEUS_IMAGE}
  #   container_name: prometheus
  #   command:
  #     - --web.console.templates=/etc/prometheus/consoles
  #     - --web.console.libraries=/etc/prometheus/console_libraries
  #     - --storage.tsdb.retention.time=1h
  #     - --config.file=/etc/prometheus/prometheus-config.yaml
  #     - --storage.tsdb.path=/prometheus
  #     - --web.enable-lifecycle
  #     - --web.route-prefix=/
  #     - --enable-feature=exemplar-storage
  #     - --enable-feature=otlp-write-receiver
  #   volumes:
  #     - ./src/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 300M
  #   restart: unless-stopped
  #   ports:
  #     - "${PROMETHEUS_SERVICE_PORT}:${PROMETHEUS_SERVICE_PORT}"
  #   logging: *logging
  #   networks:
  #     - net

  redis:
    image: bitnami/redis:6.2.10
    hostname: "goob-redis"
    restart: always
    environment:
      ALLOW_EMPTY_PASSWORD: "yes"
      REDIS_PORT_NUMBER: 7600
    healthcheck:
      test: redis-cli ping
      interval: 1s
      timeout: 3s
      retries: 50
    ports:
      - "7600:7600"
    volumes:
      - 'goob_redis_data:/bitnami/redis/data'

volumes:
  goob_redis_data:
    driver: local
  db:
