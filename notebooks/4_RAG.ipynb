{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3bc093",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is designed to guide you through the process of building a simple retrieval augmented generation application using Langchain, ChromaDB and OpenAI's GPT-4 LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc5245",
   "metadata": {},
   "source": [
    "![](../assets/4-RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ba77b",
   "metadata": {},
   "source": [
    "### RAG: Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) combines the power of language models with external content retrieval to answer questions based on specific content. This approach allows for more accurate and contextually relevant responses by leveraging a database of information.\n",
    "\n",
    "The process involves several key steps:\n",
    "- **Content Retrieval and Storage**: First, we ingest and store our content in a searchable format. This involves fetching content from a URL, splitting it into manageable parts, embedding these parts for semantic search, and storing them in a Vector DB.\n",
    "- **Question Answering**: When a user question is posed, the system retrieves relevant content from the Vector DB and uses it to generate an answer with the LLM.\n",
    "\n",
    "This notebook will walk you through setting up a RAG system using ChromaDB for content storage and retrieval, and OpenAI's GPT-4 for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94811d54",
   "metadata": {},
   "source": [
    "# Run basic example from Langchain Vectorstores Chroma page\n",
    "\n",
    "[Read more here](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7d86",
   "metadata": {},
   "source": [
    "# Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "from goob_ai import debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"example_data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.inspect(loader)\n",
    "\n",
    "rich.inspect(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.inspect(text_splitter)\n",
    "\n",
    "rich.inspect(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee8d58",
   "metadata": {},
   "source": [
    "# Persistant saving to disk example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "docs = db2.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db3.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e77236",
   "metadata": {},
   "source": [
    "# passing a Chroma client into langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "langchain_chroma = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a0d38",
   "metadata": {},
   "source": [
    "# Use Chroma in a docker container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chroma client\n",
    "import uuid\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=\"8010\", settings=Settings(allow_reset=True))\n",
    "client.reset()  # resets the database\n",
    "collection = client.create_collection(\"my_collection\")\n",
    "for doc in docs:\n",
    "    collection.add(\n",
    "        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n",
    "    )\n",
    "\n",
    "# tell LangChain to use our client and collection name\n",
    "db4 = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db4.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66c722",
   "metadata": {},
   "source": [
    "# Chromadb: Update and Delete\n",
    "\n",
    "While building toward a real application, you want to go beyond adding data, and also update and delete data.\n",
    "\n",
    "Chroma has users provide ids to simplify the bookkeeping here. ids can be the name of the file, or a combined has like filename_paragraphNumber, etc.\n",
    "\n",
    "Chroma supports all these operations - though some of them are still being integrated all the way through the LangChain interface. Additional workflow improvements will be added soon.\n",
    "\n",
    "Here is a basic example showing how to do various operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple ids\n",
    "ids = [str(i) for i in range(1, len(docs) + 1)]\n",
    "\n",
    "# add data\n",
    "example_db = Chroma.from_documents(docs, embedding_function, ids=ids)\n",
    "docs = example_db.similarity_search(query)\n",
    "print(docs[0].metadata)\n",
    "\n",
    "# update the metadata for a document\n",
    "docs[0].metadata = {\n",
    "    \"source\": \"example_data/state_of_the_union.txt\",\n",
    "    \"new_value\": \"hello world\",\n",
    "}\n",
    "example_db.update_document(ids[0], docs[0])\n",
    "print(example_db._collection.get(ids=[ids[0]]))\n",
    "\n",
    "# delete the last document\n",
    "print(\"count before\", example_db._collection.count())\n",
    "example_db._collection.delete(ids=[ids[-1]])\n",
    "print(\"count after\", example_db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3db5c",
   "metadata": {},
   "source": [
    "# Use OpenAI Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "new_client = chromadb.EphemeralClient()\n",
    "openai_lc_client = Chroma.from_documents(\n",
    "    docs, embeddings, client=new_client, collection_name=\"openai_collection\"\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = openai_lc_client.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845396e",
   "metadata": {},
   "source": [
    "# Use OpenAI Embeddings w/ docker chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from goob_ai.aio_settings import AioSettings, aiosettings\n",
    "\n",
    "os.environ[\"FAKE\"] = f\"{aiosettings.openai_api_key.get_secret_value()}\"\n",
    "os.environ[\"FAKE\"]\n",
    "\n",
    "\n",
    "# rich.print(aiosettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chroma client\n",
    "import uuid\n",
    "\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613956e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=\"8010\", settings=Settings(allow_reset=True))\n",
    "# client.reset()  # resets the database\n",
    "collection = client.create_collection(\"openai_collection\")\n",
    "\n",
    "openai_lc_client = Chroma.from_documents(\n",
    "    docs, embeddings, client=client, collection_name=\"openai_collection\"\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = openai_lc_client.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# # for doc in docs:\n",
    "# #     collection.add(\n",
    "# #         ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n",
    "# #     )\n",
    "\n",
    "# # # tell LangChain to use our client and collection name\n",
    "# # db4 = Chroma(\n",
    "# #     client=client,\n",
    "# #     collection_name=\"my_collection\",\n",
    "# #     embedding_function=embedding_function,\n",
    "# # )\n",
    "# # query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# # docs = db4.similarity_search(query)\n",
    "# # print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0746dc1",
   "metadata": {},
   "source": [
    "# Other Information: Similarity search with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned distance score is cosine distance. Therefore, a lower score is better.\n",
    "docs = db.similarity_search_with_score(query)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2844f",
   "metadata": {},
   "source": [
    "# Test Goob-ai chroma_service\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baae82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eee431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: This is from GP, let's play with this later\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_chroma import Chroma\n",
    "# import bs4\n",
    "\n",
    "# # Load, chunk and index the contents of the blog.\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Split the content into manageable chunks for better retrieval.\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Embed the chunks and store them in ChromaDB for efficient retrieval.\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=AzureOpenAIEmbeddings(azure_deployment=os.environ[\"AZURE_EMBEDDINGS_DEPLOYMENT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e744f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# # Set up the RAG chain for retrieving and generating answers.\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# system_prompt = (\"\"\"\n",
    "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "# Context: {context}\n",
    "# \"\"\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# # Initialize the model with our deployment of Azure OpenAI\n",
    "# model = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"])\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7de2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is tool usage?\"\n",
    "# rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from goob_ai.services.chroma_service import ChromaService\n",
    "from goob_ai.llm_manager import LlmManager\n",
    "\n",
    "client = ChromaService.client\n",
    "test_collection_name = \"gp_demos\"\n",
    "\n",
    "db: Chroma = ChromaService.add_to_chroma(\n",
    "    path_to_document=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    collection_name=test_collection_name,\n",
    "    embedding_function=None,\n",
    ")\n",
    "\n",
    "# Set up the RAG chain for retrieving and generating answers.\n",
    "retriever: VectorStoreRetriever = db.as_retriever()\n",
    "system_prompt = (\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize the model with our deployment of Azure OpenAI\n",
    "# model = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"])\n",
    "model =  LlmManager().llm\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is tool usage?\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d3599",
   "metadata": {},
   "source": [
    "# Readthedocs collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b31e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "\u001b[32m2024-07-08 21:16:06.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.utils.file_functions\u001b[0m:\u001b[36mtree\u001b[0m:\u001b[36m623\u001b[0m - \u001b[34m\u001b[1mdirectory -> /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/services/../data/chroma/documents\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.utils.file_functions\u001b[0m:\u001b[36mtree\u001b[0m:\u001b[36m626\u001b[0m - \u001b[34m\u001b[1mdirectory -> /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/services/../data/chroma/documents\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.utils.file_functions\u001b[0m:\u001b[36mtree\u001b[0m:\u001b[36m628\u001b[0m - \u001b[34m\u001b[1mdirectory -> /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/services/../data/chroma/documents\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mLoading document: /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/data/chroma/documents/opencv-tutorial-readthedocs-io-en-latest.pdf\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.599\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36madd_to_chroma\u001b[0m:\u001b[36m495\u001b[0m - \u001b[34m\u001b[1mpath_to_document = /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/data/chroma/documents/opencv-tutorial-readthedocs-io-en-latest.pdf\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.599\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36madd_to_chroma\u001b[0m:\u001b[36m496\u001b[0m - \u001b[34m\u001b[1mcollection_name = readthedocs\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36madd_to_chroma\u001b[0m:\u001b[36m497\u001b[0m - \u001b[34m\u001b[1membedding_function = None\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36mget_suffix\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mext: .pdf, ext_without_period: pdf\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36mget_suffix\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mext: .pdf, ext_without_period: pdf\u001b[0m\n",
      "\u001b[32m2024-07-08 21:16:06.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoob_ai.services.chroma_service\u001b[0m:\u001b[36mget_rag_loader\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mselected filetype UNKNOWN, using None. uri: /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/data/chroma/documents/opencv-tutorial-readthedocs-io-en-latest.pdf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ /Users/malcolm/dev/bossjones/goob_ai/src/goob_ai/services/../data/chroma/documents\n",
      "    + opencv-tutorial-readthedocs-io-en-latest.pdf\n",
      "    + pillow-readthedocs-io-en-latest.pdf\n",
      "    + rich-readthedocs-io-en-latest.pdf\n",
      "    + state_of_the_union.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m     16\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mChromaService\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_to_chroma\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_to_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m     25\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[1;32m     26\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m     27\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mtest_collection_name,\n\u001b[1;32m     28\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/dev/bossjones/goob_ai/src/goob_ai/services/chroma_service.py:505\u001b[0m, in \u001b[0;36mChromaService.add_to_chroma\u001b[0;34m(path_to_document, collection_name, embedding_function)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# load the document and split it into chunks\u001b[39;00m\n\u001b[1;32m    504\u001b[0m loader: TextLoader \u001b[38;5;241m|\u001b[39m PyMuPDFLoader \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m get_rag_loader(path_to_document)\n\u001b[0;32m--> 505\u001b[0m documents: List[Document] \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m()\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# If filetype is txt, split it into chunks\u001b[39;00m\n\u001b[1;32m    508\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m get_rag_splitter(path_to_document)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "from goob_ai.services.chroma_service import ChromaService, DATA_PATH, CHROMA_PATH\n",
    "from goob_ai.utils import file_functions\n",
    "from loguru import logger as LOGGER\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "client = ChromaService.client\n",
    "test_collection_name = \"readthedocs\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "d = file_functions.tree(DATA_PATH)\n",
    "result = file_functions.filter_pdfs(d)\n",
    "\n",
    "for filename in result:\n",
    "    LOGGER.info(f\"Loading document: {filename}\")\n",
    "    db = ChromaService.add_to_chroma(\n",
    "        path_to_document=f\"{filename}\",\n",
    "        collection_name=test_collection_name,\n",
    "        embedding_function=None,\n",
    "    )\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "db = Chroma(\n",
    "    client=client,\n",
    "    collection_name=test_collection_name,\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "\n",
    "\n",
    "# query it\n",
    "query = \"How do I enable syntax highlighting with rich?\"\n",
    "docs = db.similarity_search(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
