{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3bc093",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is designed to guide you through the process of building a simple retrieval augmented generation application using Langchain, ChromaDB and OpenAI's GPT-4 LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc5245",
   "metadata": {},
   "source": [
    "![](../assets/4-RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ba77b",
   "metadata": {},
   "source": [
    "### RAG: Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) combines the power of language models with external content retrieval to answer questions based on specific content. This approach allows for more accurate and contextually relevant responses by leveraging a database of information.\n",
    "\n",
    "The process involves several key steps:\n",
    "- **Content Retrieval and Storage**: First, we ingest and store our content in a searchable format. This involves fetching content from a URL, splitting it into manageable parts, embedding these parts for semantic search, and storing them in a Vector DB.\n",
    "- **Question Answering**: When a user question is posed, the system retrieves relevant content from the Vector DB and uses it to generate an answer with the LLM.\n",
    "\n",
    "This notebook will walk you through setting up a RAG system using ChromaDB for content storage and retrieval, and OpenAI's GPT-4 for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f5a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\".envrc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94811d54",
   "metadata": {},
   "source": [
    "# Run basic example from Langchain Vectorstores Chroma page\n",
    "\n",
    "[Read more here](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7d86",
   "metadata": {},
   "source": [
    "# Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc8f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.10.14)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/malcolm/dev/bossjones/goob_ai/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"example_data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75eee431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# # NOTE: This is from GP, let's play with this later\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_chroma import Chroma\n",
    "# import bs4\n",
    "\n",
    "# # Load, chunk and index the contents of the blog.\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Split the content into manageable chunks for better retrieval.\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Embed the chunks and store them in ChromaDB for efficient retrieval.\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=AzureOpenAIEmbeddings(azure_deployment=os.environ[\"AZURE_EMBEDDINGS_DEPLOYMENT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e744f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# # Set up the RAG chain for retrieving and generating answers.\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# system_prompt = (\"\"\"\n",
    "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "# Context: {context}\n",
    "# \"\"\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# # Initialize the model with our deployment of Azure OpenAI\n",
    "# model = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"])\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7de2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool usage is the ability to create, modify, and utilize external objects to perform tasks beyond one's physical and cognitive limits. For language models (LLMs), tool usage involves calling external APIs to access additional information, execute code, or retrieve proprietary data. This capability significantly extends the model's functionality by allowing it to interact with external systems and perform complex, multi-step tasks.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"What is tool usage?\"\n",
    "# rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
