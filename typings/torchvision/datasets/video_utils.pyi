"""
This type stub file was generated by pyright.
"""

import torch
from fractions import Fraction
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union

T = TypeVar("T")
def pts_convert(pts: int, timebase_from: Fraction, timebase_to: Fraction, round_func: Callable = ...) -> int:
    """convert pts between different time bases
    Args:
        pts: presentation timestamp, float
        timebase_from: original timebase. Fraction
        timebase_to: new timebase. Fraction
        round_func: rounding function.
    """
    ...

def unfold(tensor: torch.Tensor, size: int, step: int, dilation: int = ...) -> torch.Tensor:
    """
    similar to tensor.unfold, but with the dilation
    and specialized for 1d tensors

    Returns all consecutive windows of `size` elements, with
    `step` between windows. The distance between each element
    in a window is given by `dilation`.
    """
    ...

class _VideoTimestampsDataset:
    """
    Dataset used to parallelize the reading of the timestamps
    of a list of videos, given their paths in the filesystem.

    Used in VideoClips and defined at top level, so it can be
    pickled when forking.
    """
    def __init__(self, video_paths: List[str]) -> None:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __getitem__(self, idx: int) -> Tuple[List[int], Optional[float]]:
        ...
    


class VideoClips:
    """
    Given a list of video files, computes all consecutive subvideos of size
    `clip_length_in_frames`, where the distance between each subvideo in the
    same video is defined by `frames_between_clips`.
    If `frame_rate` is specified, it will also resample all the videos to have
    the same frame rate, and the clips will refer to this frame rate.

    Creating this instance the first time is time-consuming, as it needs to
    decode all the videos in `video_paths`. It is recommended that you
    cache the results after instantiation of the class.

    Recreating the clips for different clip lengths is fast, and can be done
    with the `compute_clips` method.

    Args:
        video_paths (List[str]): paths to the video files
        clip_length_in_frames (int): size of a clip in number of frames
        frames_between_clips (int): step (in frames) between each clip
        frame_rate (float, optional): if specified, it will resample the video
            so that it has `frame_rate`, and then the clips will be defined
            on the resampled video
        num_workers (int): how many subprocesses to use for data loading.
            0 means that the data will be loaded in the main process. (default: 0)
        output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
    """
    def __init__(self, video_paths: List[str], clip_length_in_frames: int = ..., frames_between_clips: int = ..., frame_rate: Optional[float] = ..., _precomputed_metadata: Optional[Dict[str, Any]] = ..., num_workers: int = ..., _video_width: int = ..., _video_height: int = ..., _video_min_dimension: int = ..., _video_max_dimension: int = ..., _audio_samples: int = ..., _audio_channels: int = ..., output_format: str = ...) -> None:
        ...
    
    @property
    def metadata(self) -> Dict[str, Any]:
        ...
    
    def subset(self, indices: List[int]) -> VideoClips:
        ...
    
    @staticmethod
    def compute_clips_for_video(video_pts: torch.Tensor, num_frames: int, step: int, fps: Optional[float], frame_rate: Optional[float] = ...) -> Tuple[torch.Tensor, Union[List[slice], torch.Tensor]]:
        ...
    
    def compute_clips(self, num_frames: int, step: int, frame_rate: Optional[float] = ...) -> None:
        """
        Compute all consecutive sequences of clips from video_pts.
        Always returns clips of size `num_frames`, meaning that the
        last few frames in a video can potentially be dropped.

        Args:
            num_frames (int): number of frames for the clip
            step (int): distance between two clips
            frame_rate (int, optional): The frame rate
        """
        ...
    
    def __len__(self) -> int:
        ...
    
    def num_videos(self) -> int:
        ...
    
    def num_clips(self) -> int:
        """
        Number of subclips that are available in the video list.
        """
        ...
    
    def get_clip_location(self, idx: int) -> Tuple[int, int]:
        """
        Converts a flattened representation of the indices into a video_idx, clip_idx
        representation.
        """
        ...
    
    def get_clip(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, Any], int]:
        """
        Gets a subclip from a list of videos.

        Args:
            idx (int): index of the subclip. Must be between 0 and num_clips().

        Returns:
            video (Tensor)
            audio (Tensor)
            info (Dict)
            video_idx (int): index of the video in `video_paths`
        """
        ...
    
    def __getstate__(self) -> Dict[str, Any]:
        ...
    
    def __setstate__(self, d: Dict[str, Any]) -> None:
        ...
    


