"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple, Type, TypeVar
from torch.types import _device, _dtype, _size

D = TypeVar("D", bound="TVTensor")
class TVTensor(torch.Tensor):
    """Base class for all TVTensors.

    You probably don't want to use this class unless you're defining your own
    custom TVTensors. See
    :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
    """
    @classmethod
    def __torch_function__(cls, func: Callable[..., torch.Tensor], types: Tuple[Type[torch.Tensor], ...], args: Sequence[Any] = ..., kwargs: Optional[Mapping[str, Any]] = ...) -> torch.Tensor:
        """For general information about how the __torch_function__ protocol works,
        see https://pytorch.org/docs/stable/notes/extending.html#extending-torch

        TL;DR: Every time a PyTorch operator is called, it goes through the inputs and looks for the
        ``__torch_function__`` method. If one is found, it is invoked with the operator as ``func`` as well as the
        ``args`` and ``kwargs`` of the original call.

        Why do we override this? Because the base implementation in torch.Tensor would preserve the TVTensor type
        of the output. In our case, we want to return pure tensors instead (with a few exceptions). Refer to the
        "TVTensors FAQ" gallery example for a rationale of this behaviour (TL;DR: perf + no silver bullet).

        Our implementation below is very similar to the base implementation in ``torch.Tensor`` - go check it out.
        """
        ...
    
    @property
    def shape(self) -> _size:
        ...
    
    @property
    def ndim(self) -> int:
        ...
    
    @property
    def device(self, *args: Any, **kwargs: Any) -> _device:
        ...
    
    @property
    def dtype(self) -> _dtype:
        ...
    
    def __deepcopy__(self: D, memo: Dict[int, Any]) -> D:
        ...
    


