"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from typing import Optional
from .._api import WeightsEnum, register_model
from .._utils import handle_legacy_interface

__all__ = ("RAFT", "raft_large", "raft_small", "Raft_Large_Weights", "Raft_Small_Weights")
class ResidualBlock(nn.Module):
    """Slightly modified Residual block with extra relu and biases."""
    def __init__(self, in_channels, out_channels, *, norm_layer, stride=..., always_project: bool = ...) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class BottleneckBlock(nn.Module):
    """Slightly modified BottleNeck block (extra relu and biases)"""
    def __init__(self, in_channels, out_channels, *, norm_layer, stride=...) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class FeatureEncoder(nn.Module):
    """The feature encoder, used both as the actual feature encoder, and as the context encoder.

    It must downsample its input by 8.
    """
    def __init__(self, *, block=..., layers=..., strides=..., norm_layer=...) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class MotionEncoder(nn.Module):
    """The motion encoder, part of the update block.

    Takes the current predicted flow and the correlation features as input and returns an encoded version of these.
    """
    def __init__(self, *, in_channels_corr, corr_layers=..., flow_layers=..., out_channels=...) -> None:
        ...
    
    def forward(self, flow, corr_features): # -> Tensor:
        ...
    


class ConvGRU(nn.Module):
    """Convolutional Gru unit."""
    def __init__(self, *, input_size, hidden_size, kernel_size, padding) -> None:
        ...
    
    def forward(self, h, x):
        ...
    


class RecurrentBlock(nn.Module):
    """Recurrent block, part of the update block.

    Takes the current hidden state and the concatenation of (motion encoder output, context) as input.
    Returns an updated hidden state.
    """
    def __init__(self, *, input_size, hidden_size, kernel_size=..., padding=...) -> None:
        ...
    
    def forward(self, h, x): # -> Any:
        ...
    


class FlowHead(nn.Module):
    """Flow head, part of the update block.

    Takes the hidden state of the recurrent unit as input, and outputs the predicted "delta flow".
    """
    def __init__(self, *, in_channels, hidden_size) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class UpdateBlock(nn.Module):
    """The update block which contains the motion encoder, the recurrent block, and the flow head.

    It must expose a ``hidden_state_size`` attribute which is the hidden state size of its recurrent block.
    """
    def __init__(self, *, motion_encoder, recurrent_block, flow_head) -> None:
        ...
    
    def forward(self, hidden_state, context, corr_features, flow): # -> tuple[Any, Any]:
        ...
    


class MaskPredictor(nn.Module):
    """Mask predictor to be used when upsampling the predicted flow.

    It takes the hidden state of the recurrent unit as input and outputs the mask.
    This is not used in the raft-small model.
    """
    def __init__(self, *, in_channels, hidden_size, multiplier=...) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class CorrBlock(nn.Module):
    """The correlation block.

    Creates a correlation pyramid with ``num_levels`` levels from the outputs of the feature encoder,
    and then indexes from this pyramid to create correlation features.
    The "indexing" of a given centroid pixel x' is done by concatenating its surrounding neighbors that
    are within a ``radius``, according to the infinity norm (see paper section 3.2).
    Note: typo in the paper, it should be infinity norm, not 1-norm.
    """
    def __init__(self, *, num_levels: int = ..., radius: int = ...) -> None:
        ...
    
    def build_pyramid(self, fmap1, fmap2): # -> None:
        """Build the correlation pyramid from two feature maps.

        The correlation volume is first computed as the dot product of each pair (pixel_in_fmap1, pixel_in_fmap2)
        The last 2 dimensions of the correlation volume are then pooled num_levels times at different resolutions
        to build the correlation pyramid.
        """
        ...
    
    def index_pyramid(self, centroids_coords): # -> Tensor:
        """Return correlation features by indexing from the pyramid."""
        ...
    


class RAFT(nn.Module):
    def __init__(self, *, feature_encoder, context_encoder, corr_block, update_block, mask_predictor=...) -> None:
        """RAFT model from
        `RAFT: Recurrent All Pairs Field Transforms for Optical Flow <https://arxiv.org/abs/2003.12039>`_.

        args:
            feature_encoder (nn.Module): The feature encoder. It must downsample the input by 8.
                Its input is the concatenation of ``image1`` and ``image2``.
            context_encoder (nn.Module): The context encoder. It must downsample the input by 8.
                Its input is ``image1``. As in the original implementation, its output will be split into 2 parts:

                - one part will be used as the actual "context", passed to the recurrent unit of the ``update_block``
                - one part will be used to initialize the hidden state of the recurrent unit of
                  the ``update_block``

                These 2 parts are split according to the ``hidden_state_size`` of the ``update_block``, so the output
                of the ``context_encoder`` must be strictly greater than ``hidden_state_size``.

            corr_block (nn.Module): The correlation block, which creates a correlation pyramid from the output of the
                ``feature_encoder``, and then indexes from this pyramid to create correlation features. It must expose
                2 methods:

                - a ``build_pyramid`` method that takes ``feature_map_1`` and ``feature_map_2`` as input (these are the
                  output of the ``feature_encoder``).
                - a ``index_pyramid`` method that takes the coordinates of the centroid pixels as input, and returns
                  the correlation features. See paper section 3.2.

                It must expose an ``out_channels`` attribute.

            update_block (nn.Module): The update block, which contains the motion encoder, the recurrent unit, and the
                flow head. It takes as input the hidden state of its recurrent unit, the context, the correlation
                features, and the current predicted flow. It outputs an updated hidden state, and the ``delta_flow``
                prediction (see paper appendix A). It must expose a ``hidden_state_size`` attribute.
            mask_predictor (nn.Module, optional): Predicts the mask that will be used to upsample the predicted flow.
                The output channel must be 8 * 8 * 9 - see paper section 3.3, and Appendix B.
                If ``None`` (default), the flow is upsampled using interpolation.
        """
        ...
    
    def forward(self, image1, image2, num_flow_updates: int = ...): # -> list[Any]:
        ...
    


_COMMON_META = ...
class Raft_Large_Weights(WeightsEnum):
    """The metrics reported here are as follows.

    ``epe`` is the "end-point-error" and indicates how far (in pixels) the
    predicted flow is from its true value. This is averaged over all pixels
    of all images. ``per_image_epe`` is similar, but the average is different:
    the epe is first computed on each image independently, and then averaged
    over all images. This corresponds to "Fl-epe" (sometimes written "F1-epe")
    in the original paper, and it's only used on Kitti. ``fl-all`` is also a
    Kitti-specific metric, defined by the author of the dataset and used for the
    Kitti leaderboard. It corresponds to the average of pixels whose epe is
    either <3px, or <5% of flow's 2-norm.
    """
    C_T_V1 = ...
    C_T_V2 = ...
    C_T_SKHT_V1 = ...
    C_T_SKHT_V2 = ...
    C_T_SKHT_K_V1 = ...
    C_T_SKHT_K_V2 = ...
    DEFAULT = ...


class Raft_Small_Weights(WeightsEnum):
    """The metrics reported here are as follows.

    ``epe`` is the "end-point-error" and indicates how far (in pixels) the
    predicted flow is from its true value. This is averaged over all pixels
    of all images. ``per_image_epe`` is similar, but the average is different:
    the epe is first computed on each image independently, and then averaged
    over all images. This corresponds to "Fl-epe" (sometimes written "F1-epe")
    in the original paper, and it's only used on Kitti. ``fl-all`` is also a
    Kitti-specific metric, defined by the author of the dataset and used for the
    Kitti leaderboard. It corresponds to the average of pixels whose epe is
    either <3px, or <5% of flow's 2-norm.
    """
    C_T_V1 = ...
    C_T_V2 = ...
    DEFAULT = ...


@register_model()
@handle_legacy_interface(weights=("pretrained", Raft_Large_Weights.C_T_SKHT_V2))
def raft_large(*, weights: Optional[Raft_Large_Weights] = ..., progress=..., **kwargs) -> RAFT:
    """RAFT model from
    `RAFT: Recurrent All Pairs Field Transforms for Optical Flow <https://arxiv.org/abs/2003.12039>`_.

    Please see the example below for a tutorial on how to use this model.

    Args:
        weights(:class:`~torchvision.models.optical_flow.Raft_Large_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.optical_flow.Raft_Large_Weights`
            below for more details, and possible values. By default, no
            pre-trained weights are used.
        progress (bool): If True, displays a progress bar of the download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.optical_flow.RAFT``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_
            for more details about this class.

    .. autoclass:: torchvision.models.optical_flow.Raft_Large_Weights
        :members:
    """
    ...

@register_model()
@handle_legacy_interface(weights=("pretrained", Raft_Small_Weights.C_T_V2))
def raft_small(*, weights: Optional[Raft_Small_Weights] = ..., progress=..., **kwargs) -> RAFT:
    """RAFT "small" model from
    `RAFT: Recurrent All Pairs Field Transforms for Optical Flow <https://arxiv.org/abs/2003.12039>`__.

    Please see the example below for a tutorial on how to use this model.

    Args:
        weights(:class:`~torchvision.models.optical_flow.Raft_Small_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.models.optical_flow.Raft_Small_Weights`
            below for more details, and possible values. By default, no
            pre-trained weights are used.
        progress (bool): If True, displays a progress bar of the download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.models.optical_flow.RAFT``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_
            for more details about this class.

    .. autoclass:: torchvision.models.optical_flow.Raft_Small_Weights
        :members:
    """
    ...

