"""
This type stub file was generated by pyright.
"""

from typing import Dict, List, Tuple
from tokenizers import Tokenizer

"""
Utilities to convert slow tokenizers in their fast tokenizers counterparts.

All the conversions are grouped here to gather SentencePiece dependencies outside of the fast tokenizers files and
allow to make our dependency on SentencePiece optional.
"""
def import_protobuf(error_message=...): # -> Any:
    ...

class SentencePieceExtractor:
    """
    Extractor implementation for SentencePiece trained models. https://github.com/google/sentencepiece
    """
    def __init__(self, model: str) -> None:
        ...
    
    def extract(self, vocab_scores=...) -> Tuple[Dict[str, int], List[Tuple]]:
        """
        By default will return vocab and merges with respect to their order, by sending `vocab_scores` we're going to
        order the merges with respect to the piece scores instead.
        """
        ...
    


class GemmaSentencePieceExtractor(SentencePieceExtractor):
    def extract(self, vocab_scores=...) -> Tuple[Dict[str, int], List[Tuple]]:
        """
        By default will return vocab and merges with respect to their order, by sending `vocab_scores` we're going to
        order the merges with respect to the piece scores instead.
        """
        ...
    


def check_number_comma(piece: str) -> bool:
    ...

class Converter:
    def __init__(self, original_tokenizer) -> None:
        ...
    
    def converted(self) -> Tokenizer:
        ...
    


class BertConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class SplinterConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class FunnelConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class MPNetConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class OpenAIGPTConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class GPT2Converter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class HerbertConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class Qwen2Converter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class RobertaConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class RoFormerConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class DebertaConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class SpmConverter(Converter):
    def __init__(self, *args) -> None:
        ...
    
    def vocab(self, proto): # -> list[tuple[Any, Any]]:
        ...
    
    def unk_id(self, proto):
        ...
    
    def tokenizer(self, proto): # -> Tokenizer:
        ...
    
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def pre_tokenizer(self, replacement, add_prefix_space): # -> Metaspace:
        ...
    
    def post_processor(self): # -> None:
        ...
    
    def decoder(self, replacement, add_prefix_space): # -> Metaspace:
        ...
    
    def converted(self) -> Tokenizer:
        ...
    


class AlbertConverter(SpmConverter):
    def vocab(self, proto): # -> list[tuple[Any, Any]]:
        ...
    
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class BarthezConverter(SpmConverter):
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class CamembertConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class DebertaV2Converter(SpmConverter):
    def pre_tokenizer(self, replacement, add_prefix_space): # -> Sequence:
        ...
    
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class MBartConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class MBart50Converter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class NllbConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class SeamlessM4TConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto):
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class XLMRobertaConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class XLNetConverter(SpmConverter):
    def vocab(self, proto): # -> list[tuple[Any, Any]]:
        ...
    
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class ReformerConverter(SpmConverter):
    ...


class RemBertConverter(SpmConverter):
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class BertGenerationConverter(SpmConverter):
    ...


class PegasusConverter(SpmConverter):
    def vocab(self, proto): # -> list[tuple[Any, float]]:
        ...
    
    def unk_id(self, proto):
        ...
    
    def pre_tokenizer(self, replacement, add_prefix_space): # -> Sequence:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class T5Converter(SpmConverter):
    def vocab(self, proto): # -> list[tuple[Any, Any]]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class UdopConverter(SpmConverter):
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class WhisperConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class BigBirdConverter(SpmConverter):
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class CLIPConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class LayoutLMv2Converter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class BlenderbotConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


class XGLMConverter(SpmConverter):
    def vocab(self, proto): # -> list[Any]:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def post_processor(self): # -> TemplateProcessing:
        ...
    


class GemmaConvert(SpmConverter):
    handle_byte_fallback = ...
    def normalizer(self, proto): # -> Replace:
        ...
    
    def vocab(self, proto): # -> list[tuple[Any, float]]:
        ...
    
    def pre_tokenizer(self, replacement, add_prefix_space): # -> None:
        ...
    
    def unk_id(self, proto): # -> Literal[3]:
        ...
    
    def decoder(self, replacement, add_prefix_space): # -> Sequence:
        ...
    
    def tokenizer(self, proto): # -> Tokenizer:
        ...
    


class LlamaConverter(SpmConverter):
    handle_byte_fallback = ...
    def vocab(self, proto): # -> list[tuple[Any, float]]:
        ...
    
    def unk_id(self, proto): # -> Literal[0]:
        ...
    
    def decoder(self, replacement, add_prefix_space): # -> Sequence:
        ...
    
    def tokenizer(self, proto): # -> Tokenizer:
        ...
    
    def normalizer(self, proto): # -> Sequence:
        ...
    
    def pre_tokenizer(self, replacement, add_prefix_space): # -> None:
        ...
    
    def post_processor(self): # -> None:
        ...
    


class MarkupLMConverter(Converter):
    def converted(self) -> Tokenizer:
        ...
    


SLOW_TO_FAST_CONVERTERS = ...
def convert_slow_tokenizer(transformer_tokenizer) -> Tokenizer:
    """
    Utilities to convert a slow tokenizer instance in a fast tokenizer instance.

    Args:
        transformer_tokenizer ([`~tokenization_utils_base.PreTrainedTokenizer`]):
            Instance of a slow tokenizer to convert in the backend tokenizer for
            [`~tokenization_utils_base.PreTrainedTokenizerFast`].

    Return:
        A instance of [`~tokenizers.Tokenizer`] to be used as the backend tokenizer of a
        [`~tokenization_utils_base.PreTrainedTokenizerFast`]
    """
    ...

