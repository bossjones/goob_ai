"""
This type stub file was generated by pyright.
"""

from typing import Optional, TYPE_CHECKING
from ..models.auto import AutoTokenizer

if TYPE_CHECKING:
    ...
class BaseStreamer:
    """
    Base class from which `.generate()` streamers should inherit.
    """
    def put(self, value):
        """Function that is called by `.generate()` to push new tokens"""
        ...
    
    def end(self):
        """Function that is called by `.generate()` to signal the end of generation"""
        ...
    


class TextStreamer(BaseStreamer):
    """
    Simple text streamer that prints the token(s) to stdout as soon as entire words are formed.

    <Tip warning={true}>

    The API for the streamer classes is still under development and may change in the future.

    </Tip>

    Parameters:
        tokenizer (`AutoTokenizer`):
            The tokenized used to decode the tokens.
        skip_prompt (`bool`, *optional*, defaults to `False`):
            Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.
        decode_kwargs (`dict`, *optional*):
            Additional keyword arguments to pass to the tokenizer's `decode` method.

    Examples:

        ```python
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

        >>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
        >>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
        >>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
        >>> streamer = TextStreamer(tok)

        >>> # Despite returning the usual output, the streamer will also print the generated text to stdout.
        >>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
        An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
        ```
    """
    def __init__(self, tokenizer: AutoTokenizer, skip_prompt: bool = ..., **decode_kwargs) -> None:
        ...
    
    def put(self, value): # -> None:
        """
        Receives tokens, decodes them, and prints them to stdout as soon as they form entire words.
        """
        ...
    
    def end(self): # -> None:
        """Flushes any remaining cache and prints a newline to stdout."""
        ...
    
    def on_finalized_text(self, text: str, stream_end: bool = ...): # -> None:
        """Prints the new text to stdout. If the stream is ending, also prints a newline."""
        ...
    


class TextIteratorStreamer(TextStreamer):
    """
    Streamer that stores print-ready text in a queue, to be used by a downstream application as an iterator. This is
    useful for applications that benefit from acessing the generated text in a non-blocking way (e.g. in an interactive
    Gradio demo).

    <Tip warning={true}>

    The API for the streamer classes is still under development and may change in the future.

    </Tip>

    Parameters:
        tokenizer (`AutoTokenizer`):
            The tokenized used to decode the tokens.
        skip_prompt (`bool`, *optional*, defaults to `False`):
            Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.
        timeout (`float`, *optional*):
            The timeout for the text queue. If `None`, the queue will block indefinitely. Useful to handle exceptions
            in `.generate()`, when it is called in a separate thread.
        decode_kwargs (`dict`, *optional*):
            Additional keyword arguments to pass to the tokenizer's `decode` method.

    Examples:

        ```python
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
        >>> from threading import Thread

        >>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
        >>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
        >>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
        >>> streamer = TextIteratorStreamer(tok)

        >>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.
        >>> generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)
        >>> thread = Thread(target=model.generate, kwargs=generation_kwargs)
        >>> thread.start()
        >>> generated_text = ""
        >>> for new_text in streamer:
        ...     generated_text += new_text
        >>> generated_text
        'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'
        ```
    """
    def __init__(self, tokenizer: AutoTokenizer, skip_prompt: bool = ..., timeout: Optional[float] = ..., **decode_kwargs) -> None:
        ...
    
    def on_finalized_text(self, text: str, stream_end: bool = ...): # -> None:
        """Put the new text in the queue. If the stream is ending, also put a stop signal in the queue."""
        ...
    
    def __iter__(self): # -> Self:
        ...
    
    def __next__(self):
        ...
    


