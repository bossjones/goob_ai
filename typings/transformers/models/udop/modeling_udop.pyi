"""
This type stub file was generated by pyright.
"""

import torch
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, Optional, Sequence, Tuple, Union
from torch import Tensor, nn
from transformers import UdopConfig
from transformers.modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings

""" PyTorch UDOP model."""
logger = ...
_CONFIG_FOR_DOC = ...
UDOP_START_DOCSTRING = ...
UDOP_INPUTS_DOCSTRING = ...
UDOP_ENCODER_INPUTS_DOCSTRING = ...
@dataclass
class BaseModelOutputWithAttentionMask(ModelOutput):
    """
    Class for the model's outputs that may also contain a past key/values (to speed up sequential decoding). Includes
    an additional attention mask.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model. If `past_key_values` is used only
            the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or
        when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if
            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,
            encoder_sequence_length, embed_size_per_head)`. Contains pre-computed hidden-states (key and values in the
            self-attention blocks and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
            that can be used (see `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or
        when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of
            the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when
        `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in
            the self-attention heads.
        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and
        `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,
            used to compute the weighted average in the cross-attention heads.
    """
    last_hidden_state: torch.FloatTensor = ...
    attention_mask: torch.FloatTensor = ...
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    attentions: Optional[Tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[Tuple[torch.FloatTensor]] = ...


def get_visual_bbox(image_size=..., patch_size=...): # -> Tensor:
    ...

def pad_sequence(seq, target_len, pad_value=...): # -> Tensor:
    ...

def combine_image_text_embeddings(image_embeddings, inputs_embeds, bbox, visual_bbox, attention_mask=..., num_patches=..., max_len=..., image_size=..., patch_size=...): # -> tuple[Tensor, Tensor, Any | None]:
    """
    Combine the image and text embeddings for the input to the encoder/decoder of UDOP.

    First, the image embeddings are created by checking for each visual patch if it is inside the bounding box of a
    token. If it is, the visual patch is combined with the token embedding. Then, the visual bounding boxes are combined
    with the text bounding boxes. Finally, the visual bounding boxes are combined with the text attention mask.
    """
    ...

class UdopPatchEmbeddings(nn.Module):
    """2D Image to Patch Embeddings"""
    def __init__(self, config) -> None:
        ...
    
    def forward(self, pixel_values): # -> Any:
        ...
    


class UdopPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models. Based on `T5PreTrainedModel`.
    """
    config_class = UdopConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _keep_in_fp32_modules = ...


class UdopLayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=...) -> None:
        """
        Construct a layernorm module in the Udop style. No bias and no subtraction of mean.
        """
        ...
    
    def forward(self, hidden_states):
        ...
    


class UdopDenseActDense(nn.Module):
    def __init__(self, config: UdopConfig) -> None:
        ...
    
    def forward(self, hidden_states): # -> Any:
        ...
    


class UdopDenseGatedActDense(nn.Module):
    def __init__(self, config: UdopConfig) -> None:
        ...
    
    def forward(self, hidden_states): # -> Any:
        ...
    


class UdopLayerFF(nn.Module):
    def __init__(self, config: UdopConfig) -> None:
        ...
    
    def forward(self, hidden_states):
        ...
    


class UdopAttention(nn.Module):
    def __init__(self, config: UdopConfig, has_relative_attention_bias=...) -> None:
        ...
    
    def prune_heads(self, heads): # -> None:
        ...
    
    def compute_bias(self, query_length, key_length, device=...): # -> Any:
        """Compute binned relative position bias"""
        ...
    
    def forward(self, hidden_states, mask=..., key_value_states=..., position_bias=..., past_key_value=..., layer_head_mask=..., query_length=..., use_cache=..., output_attentions=...): # -> tuple[Any, tuple[Tensor | Any, Tensor | Any] | None, Any | Tensor, Any | Tensor] | tuple[Any, tuple[Tensor | Any, Tensor | Any] | None, Any | Tensor]:
        """
        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).
        """
        ...
    


class UdopLayerSelfAttention(nn.Module):
    def __init__(self, config, has_relative_attention_bias=...) -> None:
        ...
    
    def forward(self, hidden_states, attention_mask=..., position_bias=..., layer_head_mask=..., past_key_value=..., use_cache=..., output_attentions=...): # -> Any:
        ...
    


class UdopLayerCrossAttention(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, hidden_states, key_value_states, attention_mask=..., position_bias=..., layer_head_mask=..., past_key_value=..., use_cache=..., query_length=..., output_attentions=...): # -> Any:
        ...
    


class UdopBlock(nn.Module):
    def __init__(self, config, has_relative_attention_bias=...) -> None:
        ...
    
    def forward(self, hidden_states, attention_mask=..., position_bias=..., encoder_hidden_states=..., encoder_attention_mask=..., encoder_decoder_position_bias=..., layer_head_mask=..., cross_attn_layer_head_mask=..., past_key_value=..., use_cache=..., output_attentions=..., return_dict=...): # -> Any:
        ...
    


class UdopCellEmbeddings(nn.Module):
    def __init__(self, max_2d_position_embeddings=..., hidden_size=...) -> None:
        ...
    
    def forward(self, bbox): # -> Any:
        ...
    


get_relative_position_bucket = ...
AUGMENTATION_RANGE = ...
class RelativePositionBiasBase(nn.Module, ABC):
    """
    Base class of relative biases.

    Args:
        num_heads (`int`):
            Number of attention heads in the model, it will create embeddings of size `num_heads`, which will be added to the scores of each token pair.
        relative_attention_num_buckets (`int`, *optional*, defaults to 32):
            Pair token metric (distance in the sequence, distance in pixels etc.) will be bucketed, parameter is defining number of such
            buckets.
        bidirectional (`bool`, *optional*, defaults to `True`):
            Whether the distance should be bidirectional for a pair of tokens. If `False`, then distance(tok1, tok2) == distance(tok2, tok1).
        scaling_factor (`int`, *optional*, defaults to 1):
            Defining factor which will be used to scale relative distance.
        max_distance (`int`, *optional*, defaults to 128):
            All distances above this value will end up in the one/same bucket.
        augmentation (`bool`, *optional*, defaults to `False`):
            Whether to multiply relative distances by a random scalar.
        expand (`bool`, *optional*, defaults to `False`):
            Whether to expand an existing pretrained model with subsequent additions of prefix_bucket.
    """
    def __init__(self, num_heads=..., relative_attention_num_buckets=..., bidirectional=..., scaling_factor=..., max_distance=..., level=..., augmentation=..., prefix_bucket=..., expand=...) -> None:
        ...
    
    @abstractmethod
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    
    def get_bucket(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    
    def get_relative_position(self, positions):
        ...
    
    def forward(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    


class RelativePositionBias1D(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None:
        """
        Reimplementation of T5 relative position bias. Distance between given tokens is their distance in the sequence.
        Parameters are the same as in base class
        """
        ...
    
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    


class RelativePositionBiasHorizontal(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None:
        """
        Represents in the bucket embeddings horizontal distance between two tokens. Parameters are the same as in base
        class
        """
        ...
    
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    


class RelativePositionBiasVertical(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None:
        """
        Represents in the bucket embeddings vertical distance between two tokens. Parameters are the same as in base
        class
        """
        ...
    
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Tensor:
        ...
    


class RelativePositionBiasAggregated(nn.Module):
    def __init__(self, modules: Sequence[RelativePositionBiasBase]) -> None:
        """
        Class which sums up various computed biases.

        Args:
            modules (Sequence[RelativePositionBiasBase]):
                List of relative bias modules.
        """
        ...
    
    def forward(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[Dict[str, Any]] = ...) -> Union[float, Tensor]:
        ...
    


BIAS_CLASSES = ...
def create_relative_bias(config: UdopConfig) -> Sequence[RelativePositionBiasBase]:
    """
    Creates empty list or one/multiple relative biases.

    :param config: Model's configuration :return: Sequence with created bias modules.
    """
    ...

class UdopStack(UdopPreTrainedModel):
    """
    This class is based on `T5Stack`, but modified to take into account the image modality as well as 2D position
    embeddings.
    """
    def __init__(self, config, embed_tokens=..., embed_patches=...) -> None:
        ...
    
    def get_input_embeddings(self): # -> None:
        ...
    
    def get_output_embeddings(self): # -> None:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    def forward(self, input_ids=..., attention_mask=..., bbox=..., encoder_hidden_states=..., encoder_attention_mask=..., inputs_embeds=..., pixel_values=..., visual_bbox=..., image_embeddings=..., position_bias=..., head_mask=..., cross_attn_head_mask=..., past_key_values=..., use_cache=..., output_attentions=..., output_hidden_states=..., return_dict=...):
        ...
    


@add_start_docstrings("The bare UDOP encoder-decoder Transformer outputting raw hidden-states without any specific head on top.", UDOP_START_DOCSTRING)
class UdopModel(UdopPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    def get_encoder(self): # -> UdopStack:
        ...
    
    def get_decoder(self): # -> UdopStack:
        ...
    
    @add_start_docstrings_to_model_forward(UDOP_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Tensor = ..., attention_mask: Tensor = ..., bbox: Dict[str, Any] = ..., pixel_values: Optional[Tensor] = ..., visual_bbox: Dict[str, Any] = ..., decoder_input_ids: Optional[Tensor] = ..., decoder_attention_mask: Optional[Tensor] = ..., inputs_embeds: Optional[Tensor] = ..., encoder_outputs: Optional[Tensor] = ..., past_key_values: Optional[Tensor] = ..., head_mask: Optional[Tensor] = ..., decoder_inputs_embeds: Optional[Tensor] = ..., decoder_head_mask: Optional[Tensor] = ..., cross_attn_head_mask: Optional[Tensor] = ..., use_cache=..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...) -> Tuple[Tensor, ...]:
        r"""
        Returns:

        Example:

        ```python
        >>> from transformers import AutoProcessor, AutoModel
        >>> from datasets import load_dataset
        >>> import torch

        >>> # load model and processor
        >>> # in this case, we already have performed OCR ourselves
        >>> # so we initialize the processor with `apply_ocr=False`
        >>> processor = AutoProcessor.from_pretrained("microsoft/udop-large", apply_ocr=False)
        >>> model = AutoModel.from_pretrained("microsoft/udop-large")

        >>> # load an example image, along with the words and coordinates
        >>> # which were extracted using an OCR engine
        >>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
        >>> example = dataset[0]
        >>> image = example["image"]
        >>> words = example["tokens"]
        >>> boxes = example["bboxes"]
        >>> inputs = processor(image, words, boxes=boxes, return_tensors="pt")

        >>> decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])

        >>> # forward pass
        >>> outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
        >>> last_hidden_states = outputs.last_hidden_state
        >>> list(last_hidden_states.shape)
        [1, 1, 1024]
        ```"""
        ...
    


@add_start_docstrings("""The UDOP encoder-decoder Transformer with a language modeling head on top, enabling to generate text given document
    images and an optional prompt.

    This class is based on [`T5ForConditionalGeneration`], extended to deal with images and layout (2D) data.""", UDOP_START_DOCSTRING)
class UdopForConditionalGeneration(UdopPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    def set_output_embeddings(self, new_embeddings): # -> None:
        ...
    
    def get_output_embeddings(self): # -> Linear:
        ...
    
    def get_encoder(self): # -> UdopStack:
        ...
    
    def get_decoder(self): # -> UdopStack:
        ...
    
    @add_start_docstrings_to_model_forward(UDOP_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Tensor = ..., attention_mask: Tensor = ..., bbox: Dict[str, Any] = ..., pixel_values: Optional[Tensor] = ..., visual_bbox: Dict[str, Any] = ..., decoder_input_ids: Optional[Tensor] = ..., decoder_attention_mask: Optional[Tensor] = ..., inputs_embeds: Optional[Tensor] = ..., encoder_outputs: Optional[Tensor] = ..., past_key_values: Optional[Tensor] = ..., head_mask: Optional[Tensor] = ..., decoder_inputs_embeds: Optional[Tensor] = ..., decoder_head_mask: Optional[Tensor] = ..., cross_attn_head_mask: Optional[Tensor] = ..., use_cache=..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., labels: Optional[Tensor] = ...) -> Tuple[Tensor, ...]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the language modeling loss. Indices should be in `[-100, 0, ..., config.vocab_size -
            1]`. All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,
            config.vocab_size]`.

        Returns:

        Examples:

        ```python
        >>> from transformers import AutoProcessor, UdopForConditionalGeneration
        >>> from datasets import load_dataset

        >>> # load model and processor
        >>> # in this case, we already have performed OCR ourselves
        >>> # so we initialize the processor with `apply_ocr=False`
        >>> processor = AutoProcessor.from_pretrained("microsoft/udop-large", apply_ocr=False)
        >>> model = UdopForConditionalGeneration.from_pretrained("microsoft/udop-large")

        >>> # load an example image, along with the words and coordinates
        >>> # which were extracted using an OCR engine
        >>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
        >>> example = dataset[0]
        >>> image = example["image"]
        >>> words = example["tokens"]
        >>> boxes = example["bboxes"]

        >>> # one can use the various task prefixes (prompts) used during pre-training
        >>> # e.g. the task prefix for DocVQA is "Question answering. "
        >>> question = "Question answering. What is the date on the form?"
        >>> encoding = processor(image, question, words, boxes=boxes, return_tensors="pt")

        >>> # autoregressive generation
        >>> predicted_ids = model.generate(**encoding)
        >>> print(processor.batch_decode(predicted_ids, skip_special_tokens=True)[0])
        9/30/92
        ```"""
        ...
    
    def prepare_inputs_for_generation(self, input_ids, past_key_values=..., attention_mask=..., head_mask=..., decoder_head_mask=..., cross_attn_head_mask=..., use_cache=..., encoder_outputs=..., **kwargs): # -> dict[str, Any]:
        ...
    


@add_start_docstrings("The bare UDOP Model transformer outputting encoder's raw hidden-states without any specific head on top.", UDOP_START_DOCSTRING)
class UdopEncoderModel(UdopPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: UdopConfig) -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    def get_encoder(self): # -> UdopStack:
        ...
    
    @add_start_docstrings_to_model_forward(UDOP_ENCODER_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=BaseModelOutputWithAttentionMask, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Tensor = ..., bbox: Dict[str, Any] = ..., attention_mask: Tensor = ..., pixel_values: Optional[Tensor] = ..., visual_bbox: Dict[str, Any] = ..., head_mask: Optional[Tensor] = ..., inputs_embeds: Optional[Tensor] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...) -> Union[Tuple[torch.FloatTensor], BaseModelOutputWithAttentionMask]:
        r"""
        Returns:

        Example:

        ```python
        >>> from transformers import AutoProcessor, UdopEncoderModel
        >>> from huggingface_hub import hf_hub_download
        >>> from datasets import load_dataset

        >>> # load model and processor
        >>> # in this case, we already have performed OCR ourselves
        >>> # so we initialize the processor with `apply_ocr=False`
        >>> processor = AutoProcessor.from_pretrained("microsoft/udop-large", apply_ocr=False)
        >>> model = UdopEncoderModel.from_pretrained("microsoft/udop-large")

        >>> # load an example image, along with the words and coordinates
        >>> # which were extracted using an OCR engine
        >>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
        >>> example = dataset[0]
        >>> image = example["image"]
        >>> words = example["tokens"]
        >>> boxes = example["bboxes"]
        >>> encoding = processor(image, words, boxes=boxes, return_tensors="pt")

        >>> outputs = model(**encoding)
        >>> last_hidden_states = outputs.last_hidden_state
        ```"""
        ...
    


