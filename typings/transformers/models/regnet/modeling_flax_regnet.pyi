"""
This type stub file was generated by pyright.
"""

import flax.linen as nn
import jax
import jax.numpy as jnp
from typing import Optional, Tuple
from flax.core.frozen_dict import FrozenDict
from transformers import RegNetConfig
from transformers.modeling_flax_outputs import FlaxBaseModelOutputWithNoAttention, FlaxBaseModelOutputWithPoolingAndNoAttention
from transformers.modeling_flax_utils import FlaxPreTrainedModel
from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward

REGNET_START_DOCSTRING = ...
REGNET_INPUTS_DOCSTRING = ...
class Identity(nn.Module):
    """Identity function."""
    @nn.compact
    def __call__(self, x, **kwargs):
        ...
    


class FlaxRegNetConvLayer(nn.Module):
    out_channels: int
    kernel_size: int = ...
    stride: int = ...
    groups: int = ...
    activation: Optional[str] = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetEmbeddings(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetShortCut(nn.Module):
    """
    RegNet shortcut, used to project the residual features to the correct size. If needed, it is also used to
    downsample the input using `stride=2`.
    """
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetSELayerCollection(nn.Module):
    in_channels: int
    reduced_channels: int
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray) -> jnp.ndarray:
        ...
    


class FlaxRegNetSELayer(nn.Module):
    """
    Squeeze and Excitation layer (SE) proposed in [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507).
    """
    in_channels: int
    reduced_channels: int
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray) -> jnp.ndarray:
        ...
    


class FlaxRegNetXLayerCollection(nn.Module):
    config: RegNetConfig
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetXLayer(nn.Module):
    """
    RegNet's layer composed by three `3x3` convolutions, same as a ResNet bottleneck layer with reduction = 1.
    """
    config: RegNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetYLayerCollection(nn.Module):
    config: RegNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray) -> jnp.ndarray:
        ...
    


class FlaxRegNetYLayer(nn.Module):
    """
    RegNet's Y layer: an X layer with Squeeze and Excitation.
    """
    config: RegNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetStageLayersCollection(nn.Module):
    """
    A RegNet stage composed by stacked layers.
    """
    config: RegNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    depth: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetStage(nn.Module):
    """
    A RegNet stage composed by stacked layers.
    """
    config: RegNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    depth: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxRegNetStageCollection(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, output_hidden_states: bool = ..., deterministic: bool = ...) -> FlaxBaseModelOutputWithNoAttention:
        ...
    


class FlaxRegNetEncoder(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, output_hidden_states: bool = ..., return_dict: bool = ..., deterministic: bool = ...) -> FlaxBaseModelOutputWithNoAttention:
        ...
    


class FlaxRegNetPreTrainedModel(FlaxPreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = RegNetConfig
    base_model_prefix = ...
    main_input_name = ...
    module_class: nn.Module = ...
    def __init__(self, config: RegNetConfig, input_shape=..., seed: int = ..., dtype: jnp.dtype = ..., _do_init: bool = ..., **kwargs) -> None:
        ...
    
    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = ...) -> FrozenDict:
        ...
    
    @add_start_docstrings_to_model_forward(REGNET_INPUTS_DOCSTRING)
    def __call__(self, pixel_values, params: dict = ..., train: bool = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...):
        ...
    


class FlaxRegNetModule(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values, deterministic: bool = ..., output_hidden_states: bool = ..., return_dict: bool = ...) -> FlaxBaseModelOutputWithPoolingAndNoAttention:
        ...
    


@add_start_docstrings("The bare RegNet model outputting raw features without any specific head on top.", REGNET_START_DOCSTRING)
class FlaxRegNetModel(FlaxRegNetPreTrainedModel):
    module_class = ...


FLAX_VISION_MODEL_DOCSTRING = ...
class FlaxRegNetClassifierCollection(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        ...
    


class FlaxRegNetForImageClassificationModule(nn.Module):
    config: RegNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values=..., deterministic: bool = ..., output_hidden_states=..., return_dict=...): # -> Any | FlaxImageClassifierOutputWithNoAttention:
        ...
    


@add_start_docstrings("""
    RegNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
    ImageNet.
    """, REGNET_START_DOCSTRING)
class FlaxRegNetForImageClassification(FlaxRegNetPreTrainedModel):
    module_class = ...


FLAX_VISION_CLASSIF_DOCSTRING = ...
