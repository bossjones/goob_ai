"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import Callable, Dict, List, Optional, Sequence, Tuple, Union
from ...modeling_outputs import ModelOutput
from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from .configuration_esm import EsmConfig
from .modeling_esm import ESM_START_DOCSTRING, EsmPreTrainedModel
from .openfold_utils import Rigid

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...
@dataclass
class EsmForProteinFoldingOutput(ModelOutput):
    """
    Output type of [`EsmForProteinFoldingOutput`].

    Args:
        frames (`torch.FloatTensor`):
            Output frames.
        sidechain_frames (`torch.FloatTensor`):
            Output sidechain frames.
        unnormalized_angles (`torch.FloatTensor`):
            Predicted unnormalized backbone and side chain torsion angles.
        angles (`torch.FloatTensor`):
            Predicted backbone and side chain torsion angles.
        positions (`torch.FloatTensor`):
            Predicted positions of the backbone and side chain atoms.
        states (`torch.FloatTensor`):
            Hidden states from the protein folding trunk.
        s_s (`torch.FloatTensor`):
            Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.
        s_z (`torch.FloatTensor`):
            Pairwise residue embeddings.
        distogram_logits (`torch.FloatTensor`):
            Input logits to the distogram used to compute residue distances.
        lm_logits (`torch.FloatTensor`):
            Logits output by the ESM-2 protein language model stem.
        aatype (`torch.FloatTensor`):
            Input amino acids (AlphaFold2 indices).
        atom14_atom_exists (`torch.FloatTensor`):
            Whether each atom exists in the atom14 representation.
        residx_atom14_to_atom37 (`torch.FloatTensor`):
            Mapping between atoms in the atom14 and atom37 representations.
        residx_atom37_to_atom14 (`torch.FloatTensor`):
            Mapping between atoms in the atom37 and atom14 representations.
        atom37_atom_exists (`torch.FloatTensor`):
            Whether each atom exists in the atom37 representation.
        residue_index (`torch.FloatTensor`):
            The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be
            a sequence of integers from 0 to `sequence_length`.
        lddt_head (`torch.FloatTensor`):
            Raw outputs from the lddt head used to compute plddt.
        plddt (`torch.FloatTensor`):
            Per-residue confidence scores. Regions of low confidence may indicate areas where the model's prediction is
            uncertain, or where the protein structure is disordered.
        ptm_logits (`torch.FloatTensor`):
            Raw logits used for computing ptm.
        ptm (`torch.FloatTensor`):
            TM-score output representing the model's high-level confidence in the overall structure.
        aligned_confidence_probs (`torch.FloatTensor`):
            Per-residue confidence scores for the aligned structure.
        predicted_aligned_error (`torch.FloatTensor`):
            Predicted error between the model's prediction and the ground truth.
        max_predicted_aligned_error (`torch.FloatTensor`):
            Per-sample maximum predicted error.
    """
    frames: torch.FloatTensor = ...
    sidechain_frames: torch.FloatTensor = ...
    unnormalized_angles: torch.FloatTensor = ...
    angles: torch.FloatTensor = ...
    positions: torch.FloatTensor = ...
    states: torch.FloatTensor = ...
    s_s: torch.FloatTensor = ...
    s_z: torch.FloatTensor = ...
    distogram_logits: torch.FloatTensor = ...
    lm_logits: torch.FloatTensor = ...
    aatype: torch.FloatTensor = ...
    atom14_atom_exists: torch.FloatTensor = ...
    residx_atom14_to_atom37: torch.FloatTensor = ...
    residx_atom37_to_atom14: torch.FloatTensor = ...
    atom37_atom_exists: torch.FloatTensor = ...
    residue_index: torch.FloatTensor = ...
    lddt_head: torch.FloatTensor = ...
    plddt: torch.FloatTensor = ...
    ptm_logits: torch.FloatTensor = ...
    ptm: torch.FloatTensor = ...
    aligned_confidence_probs: torch.FloatTensor = ...
    predicted_aligned_error: torch.FloatTensor = ...
    max_predicted_aligned_error: torch.FloatTensor = ...


ESMFOLD_INPUTS_DOCSTRING = ...
def is_fp16_enabled(): # -> _bool:
    ...

def is_deepspeed_initialized(): # -> Literal[False]:
    ...

def collate_dense_tensors(samples: List[torch.Tensor], pad_v: float = ...) -> torch.Tensor:
    """
    Takes a list of tensors with the following dimensions:
        [(d_11, ..., d_1K),
         (d_21, ..., d_2K), ..., (d_N1, ..., d_NK)]
    and stack + pads them into a single tensor of:
    (N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})
    """
    ...

def flatten_final_dims(t: torch.Tensor, no_dims: int): # -> Tensor:
    ...

def permute_final_dims(tensor: torch.Tensor, inds: List[int]): # -> Tensor:
    ...

def dict_multimap(fn, dicts): # -> dict[Any, Any]:
    ...

def trunc_normal_init_(weights, scale=..., fan=...): # -> None:
    ...

def ipa_point_weights_init_(weights): # -> None:
    ...

class EsmFoldLinear(nn.Linear):
    """
    A Linear layer with built-in nonstandard initializations. Called just like torch.nn.Linear.

    Implements the initializers in 1.11.4, plus some additional ones found in the code.
    """
    def __init__(self, in_dim: int, out_dim: int, bias: bool = ..., init: str = ..., init_fn: Optional[Callable[[torch.Tensor, torch.Tensor], None]] = ...) -> None:
        """
        Args:
            in_dim:
                The final dimension of inputs to the layer
            out_dim:
                The final dimension of layer outputs
            bias:
                Whether to learn an additive bias. True by default
            init:
                The initializer to use. Choose from:

                "default": LeCun fan-in truncated normal initialization "relu": He initialization w/ truncated normal
                distribution "glorot": Fan-average Glorot uniform initialization "gating": Weights=0, Bias=1 "normal":
                Normal initialization with std=1/sqrt(fan_in) "final": Weights=0, Bias=0

                Overridden by init_fn if the latter is not None.
            init_fn:
                A custom initializer taking weight and bias as inputs. Overrides init if not None.
        """
        ...
    


class EsmFoldLayerNorm(nn.Module):
    def __init__(self, c_in, eps=...) -> None:
        ...
    
    def forward(self, x): # -> Tensor:
        ...
    


@torch.jit.ignore
def softmax_no_cast(t: torch.Tensor, dim: int = ...) -> torch.Tensor:
    """
    Softmax, but without automatic casting to fp32 when the input is of type bfloat16
    """
    ...

class EsmFoldAttention(nn.Module):
    """
    Standard multi-head attention using AlphaFold's default layer initialization. Allows multiple bias vectors.
    """
    def __init__(self, c_q: int, c_k: int, c_v: int, c_hidden: int, no_heads: int, gating: bool = ...) -> None:
        """
        Args:
            c_q:
                Input dimension of query data
            c_k:
                Input dimension of key data
            c_v:
                Input dimension of value data
            c_hidden:
                Per-head hidden dimension
            no_heads:
                Number of attention heads
            gating:
                Whether the output should be gated using query data
        """
        ...
    
    def forward(self, q_x: torch.Tensor, kv_x: torch.Tensor, biases: Optional[List[torch.Tensor]] = ..., use_memory_efficient_kernel: bool = ..., use_lma: bool = ..., lma_q_chunk_size: int = ..., lma_kv_chunk_size: int = ..., use_flash: bool = ..., flash_mask: Optional[torch.Tensor] = ...) -> torch.Tensor:
        """
        Args:
            q_x:
                [*, Q, C_q] query data
            kv_x:
                [*, K, C_k] key data
            biases:
                List of biases that broadcast to [*, H, Q, K]
            use_memory_efficient_kernel:
                Whether to use a custom memory-efficient attention kernel. This should be the default choice for most.
                If none of the "use_<...>" flags are True, a stock PyTorch implementation is used instead
            use_lma:
                Whether to use low-memory attention (Staats & Rabe 2021). If none of the "use_<...>" flags are True, a
                stock PyTorch implementation is used instead
            lma_q_chunk_size:
                Query chunk size (for LMA)
            lma_kv_chunk_size:
                Key/Value chunk size (for LMA)
        Returns
            [*, Q, C_q] attention update
        """
        ...
    


class EsmFoldTriangleAttention(nn.Module):
    def __init__(self, c_in, c_hidden, no_heads, starting=..., inf=...) -> None:
        """
        Args:
            c_in:
                Input channel dimension
            c_hidden:
                Overall hidden channel dimension (not per-head)
            no_heads:
                Number of attention heads
        """
        ...
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = ..., chunk_size: Optional[int] = ..., use_memory_efficient_kernel: bool = ..., use_lma: bool = ..., inplace_safe: bool = ...) -> torch.Tensor:
        """
        Args:
            x:
                [*, I, J, C_in] input tensor (e.g. the pair representation)
        Returns:
            [*, I, J, C_in] output tensor
        """
        ...
    


class EsmFoldTriangleMultiplicativeUpdate(nn.Module):
    """
    Implements Algorithms 11 and 12.
    """
    def __init__(self, config, _outgoing=...) -> None:
        ...
    
    def forward(self, z: torch.Tensor, mask: Optional[torch.Tensor] = ..., inplace_safe: bool = ..., _add_with_inplace: bool = ..., _inplace_chunk_size: Optional[int] = ...) -> torch.Tensor:
        """
        Args:
            x:
                [*, N_res, N_res, C_z] input tensor
            mask:
                [*, N_res, N_res] input mask
        Returns:
            [*, N_res, N_res, C_z] output tensor
        """
        ...
    


class EsmFoldPreTrainedModel(EsmPreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    ...


class EsmFoldSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, head_width, gated=...) -> None:
        ...
    
    def forward(self, x, mask=..., bias=..., indices=...): # -> tuple[Any, Tensor]:
        """
        Basic self attention with optional mask and external pairwise bias. To handle sequences of different lengths,
        use mask.

        Inputs:
            x: batch of input sequneces (.. x L x C) mask: batch of boolean masks where 1=valid, 0=padding position (..
            x L_k) bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads)

        Outputs:
          sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)
        """
        ...
    


class EsmFoldDropout(nn.Module):
    """
    Implementation of dropout with the ability to share the dropout mask along a particular dimension.
    """
    def __init__(self, r: float, batch_dim: Union[int, List[int]]) -> None:
        ...
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        ...
    


class EsmFoldSequenceToPair(nn.Module):
    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim) -> None:
        ...
    
    def forward(self, sequence_state): # -> Any:
        """
        Inputs:
          sequence_state: B x L x sequence_state_dim

        Output:
          pairwise_state: B x L x L x pairwise_state_dim

        Intermediate state:
          B x L x L x 2*inner_dim
        """
        ...
    


class EsmFoldPairToSequence(nn.Module):
    def __init__(self, pairwise_state_dim, num_heads) -> None:
        ...
    
    def forward(self, pairwise_state): # -> Any:
        """
        Inputs:
          pairwise_state: B x L x L x pairwise_state_dim

        Output:
          pairwise_bias: B x L x L x num_heads
        """
        ...
    


class EsmFoldResidueMLP(nn.Module):
    def __init__(self, embed_dim, inner_dim, dropout=...) -> None:
        ...
    
    def forward(self, x):
        ...
    


class EsmFoldTriangularSelfAttentionBlock(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, sequence_state, pairwise_state, mask=..., chunk_size=..., **__kwargs): # -> tuple[Any, Any]:
        """
        Inputs:
          sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim mask: B x L boolean
          tensor of valid positions

        Output:
          sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim
        """
        ...
    


class EsmCategoricalMixture:
    def __init__(self, param, bins=..., start=..., end=...) -> None:
        ...
    
    def log_prob(self, true): # -> Tensor:
        ...
    
    def mean(self):
        ...
    


def categorical_lddt(logits, bins=...):
    ...

def get_axial_mask(mask): # -> None:
    """
    Helper to convert B x L mask of valid positions to axial mask used in row column attentions.

    Input:
      mask: B x L tensor of booleans

    Output:
      mask: B x L x L tensor of booleans
    """
    ...

class EsmFoldRelativePosition(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, residue_index, mask=...): # -> Any:
        """
        Input:
          residue_index: B x L tensor of indices (dytpe=torch.long) mask: B x L tensor of booleans

        Output:
          pairwise_state: B x L x L x pairwise_state_dim tensor of embeddings
        """
        ...
    


class EsmFoldAngleResnetBlock(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, a: torch.Tensor) -> torch.Tensor:
        ...
    


class EsmFoldAngleResnet(nn.Module):
    """
    Implements Algorithm 20, lines 11-14
    """
    def __init__(self, config) -> None:
        ...
    
    def forward(self, s: torch.Tensor, s_initial: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            s:
                [*, C_hidden] single embedding
            s_initial:
                [*, C_hidden] single embedding as of the start of the StructureModule
        Returns:
            [*, no_angles, 2] predicted angles
        """
        ...
    


class EsmFoldInvariantPointAttention(nn.Module):
    """
    Implements Algorithm 22.
    """
    def __init__(self, config) -> None:
        ...
    
    def forward(self, s: torch.Tensor, z: Optional[torch.Tensor], r: Rigid, mask: torch.Tensor, _offload_inference: bool = ..., _z_reference_list: Optional[Sequence[torch.Tensor]] = ...) -> torch.Tensor:
        """
        Args:
            s:
                [*, N_res, C_s] single representation
            z:
                [*, N_res, N_res, C_z] pair representation
            r:
                [*, N_res] transformation object
            mask:
                [*, N_res] mask
        Returns:
            [*, N_res, C_s] single representation update
        """
        ...
    


class EsmFoldBackboneUpdate(nn.Module):
    """
    Implements part of Algorithm 23.
    """
    def __init__(self, config) -> None:
        ...
    
    def forward(self, s: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            [*, N_res, C_s] single representation
        Returns:
            [*, N_res, 6] update vector
        """
        ...
    


class EsmFoldStructureModuleTransitionLayer(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, s):
        ...
    


class EsmFoldStructureModuleTransition(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, s): # -> Any:
        ...
    


class EsmFoldStructureModule(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, evoformer_output_dict, aatype, mask=..., _offload_inference=...): # -> dict[Any, Any]:
        """
        Args:
            evoformer_output_dict:
                Dictionary containing:
                    "single":
                        [*, N_res, C_s] single representation
                    "pair":
                        [*, N_res, N_res, C_z] pair representation
            aatype:
                [*, N_res] amino acid indices
            mask:
                Optional [*, N_res] sequence mask
        Returns:
            A dictionary of outputs
        """
        ...
    
    def torsion_angles_to_frames(self, r, alpha, f): # -> Rigid:
        ...
    
    def frames_and_literature_positions_to_atom14_pos(self, r, f): # -> Tensor:
        ...
    


class EsmFoldingTrunk(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def set_chunk_size(self, chunk_size): # -> None:
        ...
    
    def forward(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles): # -> Any:
        """
        Inputs:
          seq_feats: B x L x C tensor of sequence features pair_feats: B x L x L x C tensor of pair features residx: B
          x L long tensor giving the position in the sequence mask: B x L boolean tensor indicating valid residues

        Output:
          predicted_structure: B x L x (num_atoms_per_residue * 3) tensor wrapped in a Coordinates object
        """
        ...
    
    @staticmethod
    def distogram(coords, min_bin, max_bin, num_bins): # -> Tensor:
        ...
    


@add_start_docstrings("""
    ESMForProteinFolding is the HuggingFace port of the original ESMFold model. It consists of an ESM-2 "stem" followed
    by a protein folding "head", although unlike most other output heads, this "head" is similar in size and runtime to
    the rest of the model combined! It outputs a dictionary containing predicted structural information about the input
    protein(s).
    """, ESM_START_DOCSTRING)
class EsmForProteinFolding(EsmPreTrainedModel):
    _no_split_modules = ...
    def __init__(self, config) -> None:
        ...
    
    @add_start_docstrings_to_model_forward(ESMFOLD_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    @replace_return_docstrings(output_type=EsmForProteinFoldingOutput, config_class=EsmConfig)
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = ..., position_ids: Optional[torch.Tensor] = ..., masking_pattern: Optional[torch.Tensor] = ..., num_recycles: Optional[int] = ...) -> EsmForProteinFoldingOutput:
        r"""
        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, EsmForProteinFolding

        >>> model = EsmForProteinFolding.from_pretrained("facebook/esmfold_v1")
        >>> tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")
        >>> inputs = tokenizer(["MLKNVQVQLV"], return_tensors="pt", add_special_tokens=False)  # A tiny random peptide
        >>> outputs = model(**inputs)
        >>> folded_positions = outputs.positions
        ```

        """
        ...
    
    def af2_idx_to_esm_idx(self, aa, mask):
        ...
    
    def compute_language_model_representations(self, esmaa: torch.Tensor) -> torch.Tensor:
        ...
    
    def bert_mask(self, aa, esmaa, mask, pattern): # -> tuple[Any, Any, Any]:
        ...
    
    @torch.no_grad()
    def infer(self, seqs: Union[str, List[str]], position_ids=...): # -> EsmForProteinFoldingOutput:
        ...
    
    @staticmethod
    def output_to_pdb(output: Dict) -> List[str]:
        """Returns the pbd (file) string from the model given the model output."""
        ...
    
    def infer_pdb(self, seqs, *args, **kwargs) -> str:
        """Returns the pdb (file) string from the model given an input sequence."""
        ...
    
    def infer_pdbs(self, seqs: List[str], *args, **kwargs) -> List[str]:
        """Returns the pdb (file) string from the model given an input sequence."""
        ...
    


