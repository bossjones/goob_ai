"""
This type stub file was generated by pyright.
"""

import flax.linen as nn
import jax
import jax.numpy as jnp
from typing import Optional, Tuple
from flax.core.frozen_dict import FrozenDict
from ...modeling_flax_outputs import FlaxBaseModelOutputWithNoAttention, FlaxBaseModelOutputWithPoolingAndNoAttention
from ...modeling_flax_utils import FlaxPreTrainedModel
from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward
from .configuration_resnet import ResNetConfig

RESNET_START_DOCSTRING = ...
RESNET_INPUTS_DOCSTRING = ...
class Identity(nn.Module):
    """Identity function."""
    @nn.compact
    def __call__(self, x, **kwargs):
        ...
    


class FlaxResNetConvLayer(nn.Module):
    out_channels: int
    kernel_size: int = ...
    stride: int = ...
    activation: Optional[str] = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetEmbeddings(nn.Module):
    """
    ResNet Embeddings (stem) composed of a single aggressive convolution.
    """
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetShortCut(nn.Module):
    """
    ResNet shortcut, used to project the residual features to the correct size. If needed, it is also used to
    downsample the input using `stride=2`.
    """
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetBasicLayerCollection(nn.Module):
    out_channels: int
    stride: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetBasicLayer(nn.Module):
    """
    A classic ResNet's residual layer composed by two `3x3` convolutions.
    """
    in_channels: int
    out_channels: int
    stride: int = ...
    activation: Optional[str] = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state, deterministic: bool = ...):
        ...
    


class FlaxResNetBottleNeckLayerCollection(nn.Module):
    out_channels: int
    stride: int = ...
    activation: Optional[str] = ...
    reduction: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetBottleNeckLayer(nn.Module):
    """
    A classic ResNet's bottleneck layer composed by three `3x3` convolutions. The first `1x1` convolution reduces the
    input by a factor of `reduction` in order to make the second `3x3` convolution faster. The last `1x1` convolution
    remaps the reduced features to `out_channels`.
    """
    in_channels: int
    out_channels: int
    stride: int = ...
    activation: Optional[str] = ...
    reduction: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetStageLayersCollection(nn.Module):
    """
    A ResNet stage composed by stacked layers.
    """
    config: ResNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    depth: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetStage(nn.Module):
    """
    A ResNet stage composed by stacked layers.
    """
    config: ResNetConfig
    in_channels: int
    out_channels: int
    stride: int = ...
    depth: int = ...
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray, deterministic: bool = ...) -> jnp.ndarray:
        ...
    


class FlaxResNetStageCollection(nn.Module):
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, output_hidden_states: bool = ..., deterministic: bool = ...) -> FlaxBaseModelOutputWithNoAttention:
        ...
    


class FlaxResNetEncoder(nn.Module):
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, hidden_state: jnp.ndarray, output_hidden_states: bool = ..., return_dict: bool = ..., deterministic: bool = ...) -> FlaxBaseModelOutputWithNoAttention:
        ...
    


class FlaxResNetPreTrainedModel(FlaxPreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = ResNetConfig
    base_model_prefix = ...
    main_input_name = ...
    module_class: nn.Module = ...
    def __init__(self, config: ResNetConfig, input_shape=..., seed: int = ..., dtype: jnp.dtype = ..., _do_init: bool = ..., **kwargs) -> None:
        ...
    
    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = ...) -> FrozenDict:
        ...
    
    @add_start_docstrings_to_model_forward(RESNET_INPUTS_DOCSTRING)
    def __call__(self, pixel_values, params: dict = ..., train: bool = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...):
        ...
    


class FlaxResNetModule(nn.Module):
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values, deterministic: bool = ..., output_hidden_states: bool = ..., return_dict: bool = ...) -> FlaxBaseModelOutputWithPoolingAndNoAttention:
        ...
    


@add_start_docstrings("The bare ResNet model outputting raw features without any specific head on top.", RESNET_START_DOCSTRING)
class FlaxResNetModel(FlaxResNetPreTrainedModel):
    module_class = ...


FLAX_VISION_MODEL_DOCSTRING = ...
class FlaxResNetClassifierCollection(nn.Module):
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        ...
    


class FlaxResNetForImageClassificationModule(nn.Module):
    config: ResNetConfig
    dtype: jnp.dtype = ...
    def setup(self): # -> None:
        ...
    
    def __call__(self, pixel_values=..., deterministic: bool = ..., output_hidden_states=..., return_dict=...): # -> Any | FlaxImageClassifierOutputWithNoAttention:
        ...
    


@add_start_docstrings("""
    ResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
    ImageNet.
    """, RESNET_START_DOCSTRING)
class FlaxResNetForImageClassification(FlaxResNetPreTrainedModel):
    module_class = ...


FLAX_VISION_CLASSIF_DOCSTRING = ...
