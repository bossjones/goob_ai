"""
This type stub file was generated by pyright.
"""

import tensorflow as tf
from typing import Any, Optional, Tuple, Union
from ...modeling_tf_outputs import TFBaseModelOutput, TFCausalLMOutput
from ...modeling_tf_utils import TFPreTrainedModel, keras, keras_serializable, unpack_inputs
from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from .configuration_hubert import HubertConfig

""" TensorFlow Hubert model."""
logger = ...
_CONFIG_FOR_DOC = ...
LARGE_NEGATIVE = ...
class TFHubertGroupNorm(keras.layers.Layer):
    """
    From tensorflow-addons https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GroupNormalization
    """
    def __init__(self, groups: int = ..., axis: int = ..., epsilon: float = ..., center: bool = ..., scale: bool = ..., beta_initializer: keras.initializers.Initializer = ..., gamma_initializer: keras.initializers.Initializer = ..., beta_regularizer: keras.regularizers.Regularizer = ..., gamma_regularizer: keras.regularizers.Regularizer = ..., beta_constraint: keras.constraints.Constraint = ..., gamma_constraint: keras.constraints.Constraint = ..., **kwargs) -> None:
        ...
    
    def build(self, input_shape): # -> None:
        ...
    
    def call(self, inputs):
        ...
    
    def get_config(self): # -> dict[str, Any]:
        ...
    
    def compute_output_shape(self, input_shape):
        ...
    


class TFHubertWeightNormConv1D(keras.layers.Conv1D):
    """Adapted from https://www.tensorflow.org/probability/api_docs/python/tfp/layers/weight_norm/WeightNorm"""
    def __init__(self, filters, kernel_size, groups, explicit_padding, **kwargs) -> None:
        ...
    
    def build(self, input_shape): # -> None:
        ...
    
    def call(self, inputs):
        ...
    


class TFHubertNoLayerNormConvLayer(keras.layers.Layer):
    def __init__(self, config: HubertConfig, layer_id: int = ..., **kwargs: Any) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertLayerNormConvLayer(keras.layers.Layer):
    def __init__(self, config: HubertConfig, layer_id: int = ..., **kwargs: Any) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertGroupNormConvLayer(keras.layers.Layer):
    def __init__(self, config: HubertConfig, layer_id: int = ..., **kwargs: Any) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertPositionalConvEmbedding(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs: Any) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertSamePadLayer(keras.layers.Layer):
    def __init__(self, num_conv_pos_embeddings, **kwargs) -> None:
        ...
    
    def call(self, hidden_states):
        ...
    


class TFHubertFeatureEncoder(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs: Any) -> None:
        ...
    
    def call(self, input_values):
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertFeatureExtractor(TFHubertFeatureEncoder):
    def __init__(self, config, **kwargs) -> None:
        ...
    


class TFHubertFeatureProjection(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, training: bool = ...) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertAttention(keras.layers.Layer):
    """Multi-headed attention from "Attention Is All You Need"""
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = ..., is_decoder: bool = ..., bias: bool = ..., **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, key_value_states: tf.Tensor | None = ..., past_key_value: Tuple[Tuple[tf.Tensor]] | None = ..., attention_mask: tf.Tensor | None = ..., layer_head_mask: tf.Tensor | None = ..., training: Optional[bool] = ...) -> Tuple[tf.Tensor, tf.Tensor | None]:
        """Input shape: Batch x Time x Channel"""
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertFeedForward(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, training: bool = ...) -> tf.Tensor:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertEncoderLayer(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., training: bool = ...) -> Tuple[tf.Tensor]:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertEncoderLayerStableLayerNorm(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., training: bool = ...) -> Tuple[tf.Tensor]:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertEncoder(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., training: Optional[bool] = ...) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


class TFHubertEncoderStableLayerNorm(keras.layers.Layer):
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., training: Optional[bool] = ...) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


@keras_serializable
class TFHubertMainLayer(keras.layers.Layer):
    config_class = HubertConfig
    def __init__(self, config: HubertConfig, **kwargs) -> None:
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    
    @unpack_inputs
    def call(self, input_values: tf.Tensor, attention_mask: tf.Tensor | None = ..., token_type_ids: tf.Tensor | None = ..., position_ids: tf.Tensor | None = ..., head_mask: tf.Tensor | None = ..., inputs_embeds: tf.Tensor | None = ..., output_attentions: tf.Tensor | None = ..., output_hidden_states: tf.Tensor | None = ..., return_dict: Optional[bool] = ..., training: bool = ..., **kwargs: Any): # -> TFBaseModelOutput:
        ...
    


class TFHubertPreTrainedModel(TFPreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = HubertConfig
    base_model_prefix = ...
    main_input_name = ...
    @property
    def input_signature(self): # -> dict[str, Any]:
        ...
    
    def __init__(self, config, *inputs, **kwargs) -> None:
        ...
    


HUBERT_START_DOCSTRING = ...
HUBERT_INPUTS_DOCSTRING = ...
@add_start_docstrings("The bare TFHubert Model transformer outputing raw hidden-states without any specific head on top.", HUBERT_START_DOCSTRING)
class TFHubertModel(TFHubertPreTrainedModel):
    def __init__(self, config: HubertConfig, *inputs, **kwargs) -> None:
        ...
    
    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TFBaseModelOutput, config_class=_CONFIG_FOR_DOC)
    @unpack_inputs
    def call(self, input_values: tf.Tensor, attention_mask: tf.Tensor | None = ..., token_type_ids: tf.Tensor | None = ..., position_ids: tf.Tensor | None = ..., head_mask: tf.Tensor | None = ..., inputs_embeds: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., training: bool = ...) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:
        """

        Returns:

        Example:

        ```python
        >>> from transformers import AutoProcessor, TFHubertModel
        >>> from datasets import load_dataset
        >>> import soundfile as sf

        >>> processor = AutoProcessor.from_pretrained("facebook/hubert-large-ls960-ft")
        >>> model = TFHubertModel.from_pretrained("facebook/hubert-large-ls960-ft")


        >>> def map_to_array(batch):
        ...     speech, _ = sf.read(batch["file"])
        ...     batch["speech"] = speech
        ...     return batch


        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
        >>> ds = ds.map(map_to_array)

        >>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
        >>> hidden_states = model(input_values).last_hidden_state
        ```"""
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


@add_start_docstrings("""TFHubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).""", HUBERT_START_DOCSTRING)
class TFHubertForCTC(TFHubertPreTrainedModel):
    def __init__(self, config: HubertConfig, *inputs, **kwargs) -> None:
        ...
    
    def freeze_feature_extractor(self): # -> None:
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameters will
        not be updated during training.
        """
        ...
    
    def freeze_feature_encoder(self): # -> None:
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        ...
    
    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TFCausalLMOutput, config_class=_CONFIG_FOR_DOC)
    @unpack_inputs
    def call(self, input_values: tf.Tensor, attention_mask: tf.Tensor | None = ..., token_type_ids: tf.Tensor | None = ..., position_ids: tf.Tensor | None = ..., head_mask: tf.Tensor | None = ..., inputs_embeds: tf.Tensor | None = ..., output_attentions: Optional[bool] = ..., labels: tf.Tensor | None = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., training: Optional[bool] = ...) -> Union[TFCausalLMOutput, Tuple[tf.Tensor]]:
        r"""
        labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_values` docstring) Tokens with indices set to `-100` are ignored (masked),
            the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`

        Returns:

        Example:

        ```python
        >>> import tensorflow as tf
        >>> from transformers import AutoProcessor, TFHubertForCTC
        >>> from datasets import load_dataset
        >>> import soundfile as sf

        >>> processor = AutoProcessor.from_pretrained("facebook/hubert-large-ls960-ft")
        >>> model = TFHubertForCTC.from_pretrained("facebook/hubert-large-ls960-ft")


        >>> def map_to_array(batch):
        ...     speech, _ = sf.read(batch["file"])
        ...     batch["speech"] = speech
        ...     return batch


        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
        >>> ds = ds.map(map_to_array)

        >>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
        >>> logits = model(input_values).logits
        >>> predicted_ids = tf.argmax(logits, axis=-1)

        >>> transcription = processor.decode(predicted_ids[0])

        >>> # compute loss
        >>> target_transcription = "A MAN SAID TO THE UNIVERSE SIR I EXIST"

        >>> # Pass the transcription as text to encode labels
        >>> labels = processor(text=transcription, return_tensors="tf").input_values

        >>> loss = model(input_values, labels=labels).loss
        ```"""
        ...
    
    def build(self, input_shape=...): # -> None:
        ...
    


