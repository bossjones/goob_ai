"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Optional, Tuple, Union
from torch import nn
from ...modeling_outputs import BaseModelOutput, ModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from .configuration_vits import VitsConfig

""" PyTorch VITS model."""
logger = ...
_CONFIG_FOR_DOC = ...
@dataclass
class VitsModelOutput(ModelOutput):
    """
    Describes the outputs for the VITS model, with potential hidden states and attentions.

    Args:
        waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
            The final audio waveform predicted by the model.
        sequence_lengths  (`torch.FloatTensor` of shape `(batch_size,)`):
            The length in samples of each element in the `waveform` batch.
        spectrogram (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_bins)`):
            The log-mel spectrogram predicted at the output of the flow model. This spectrogram is passed to the Hi-Fi
            GAN decoder model to obtain the final audio waveform.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attention weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
    """
    waveform: torch.FloatTensor = ...
    sequence_lengths: torch.FloatTensor = ...
    spectrogram: Optional[Tuple[torch.FloatTensor]] = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    attentions: Optional[Tuple[torch.FloatTensor]] = ...


@dataclass
class VitsTextEncoderOutput(ModelOutput):
    """
    Describes the outputs for the VITS text encoder model, with potential hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        prior_means (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            The predicted mean values of the prior distribution for the latent text variables.
        prior_log_variances (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            The predicted log-variance values of the prior distribution for the latent text variables.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attention weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
    """
    last_hidden_state: torch.FloatTensor = ...
    prior_means: torch.FloatTensor = ...
    prior_log_variances: torch.FloatTensor = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    attentions: Optional[Tuple[torch.FloatTensor]] = ...


@torch.jit.script
def fused_add_tanh_sigmoid_multiply(input_a, input_b, num_channels): # -> Tensor:
    ...

class VitsWaveNet(torch.nn.Module):
    def __init__(self, config: VitsConfig, num_layers: int) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=...):
        ...
    
    def remove_weight_norm(self): # -> None:
        ...
    


class VitsPosteriorEncoder(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=...): # -> tuple[Any, Tensor, Tensor]:
        ...
    


class HifiGanResidualBlock(nn.Module):
    def __init__(self, channels, kernel_size=..., dilation=..., leaky_relu_slope=...) -> None:
        ...
    
    def get_padding(self, kernel_size, dilation=...):
        ...
    
    def apply_weight_norm(self): # -> None:
        ...
    
    def remove_weight_norm(self): # -> None:
        ...
    
    def forward(self, hidden_states):
        ...
    


class VitsHifiGan(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def apply_weight_norm(self): # -> None:
        ...
    
    def remove_weight_norm(self): # -> None:
        ...
    
    def forward(self, spectrogram: torch.FloatTensor, global_conditioning: Optional[torch.FloatTensor] = ...) -> torch.FloatTensor:
        r"""
        Converts a spectrogram into a speech waveform.

        Args:
            spectrogram (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins, sequence_length)`):
                Tensor containing the spectrograms.
            global_conditioning (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_size, 1)`, *optional*):
                Tensor containing speaker embeddings, for multispeaker models.

        Returns:
            `torch.FloatTensor`: Tensor of shape shape `(batch_size, 1, num_frames)` containing the speech waveform.
        """
        ...
    


class VitsResidualCouplingLayer(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=..., reverse=...): # -> tuple[Tensor, Tensor] | tuple[Tensor, None]:
        ...
    


class VitsResidualCouplingBlock(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=..., reverse=...): # -> Tensor | Any:
        ...
    


class VitsDilatedDepthSeparableConv(nn.Module):
    def __init__(self, config: VitsConfig, dropout_rate=...) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=...):
        ...
    


class VitsConvFlow(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=..., reverse=...): # -> tuple[Any, Tensor] | tuple[Any, None]:
        ...
    


class VitsElementwiseAffine(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=..., reverse=...): # -> tuple[Any, Tensor] | tuple[Any, None]:
        ...
    


class VitsStochasticDurationPredictor(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=..., durations=..., reverse=..., noise_scale=...): # -> Any | Tensor:
        ...
    


class VitsDurationPredictor(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, inputs, padding_mask, global_conditioning=...):
        ...
    


class VitsAttention(nn.Module):
    """Multi-headed attention with relative positional representation."""
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, hidden_states: torch.Tensor, key_value_states: Optional[torch.Tensor] = ..., attention_mask: Optional[torch.Tensor] = ..., layer_head_mask: Optional[torch.Tensor] = ..., output_attentions: bool = ...) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Input shape: Batch x Time x Channel"""
        ...
    


class VitsFeedForward(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, hidden_states, padding_mask):
        ...
    


class VitsEncoderLayer(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, hidden_states: torch.Tensor, padding_mask: torch.FloatTensor, attention_mask: Optional[torch.Tensor] = ..., output_attentions: bool = ...): # -> tuple[Tensor, Any] | tuple[Tensor]:
        ...
    


class VitsEncoder(nn.Module):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def forward(self, hidden_states: torch.FloatTensor, padding_mask: torch.FloatTensor, attention_mask: Optional[torch.Tensor] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...) -> Union[Tuple, BaseModelOutput]:
        ...
    


class VitsTextEncoder(nn.Module):
    """
    Transformer encoder that uses relative positional representation instead of absolute positional encoding.
    """
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, value): # -> None:
        ...
    
    def forward(self, input_ids: torch.Tensor, padding_mask: torch.FloatTensor, attention_mask: Optional[torch.Tensor] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...) -> Union[Tuple[torch.Tensor], VitsTextEncoderOutput]:
        ...
    


class VitsPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = VitsConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...


VITS_START_DOCSTRING = ...
VITS_INPUTS_DOCSTRING = ...
@add_start_docstrings("The complete VITS model, for text-to-speech synthesis.", VITS_START_DOCSTRING)
class VitsModel(VitsPreTrainedModel):
    def __init__(self, config: VitsConfig) -> None:
        ...
    
    def get_encoder(self): # -> VitsTextEncoder:
        ...
    
    @add_start_docstrings_to_model_forward(VITS_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=VitsModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Optional[torch.Tensor] = ..., attention_mask: Optional[torch.Tensor] = ..., speaker_id: Optional[int] = ..., output_attentions: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., labels: Optional[torch.FloatTensor] = ...) -> Union[Tuple[Any], VitsModelOutput]:
        r"""
        labels (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins, sequence_length)`, *optional*):
            Float values of target spectrogram. Timesteps set to `-100.0` are ignored (masked) for the loss
            computation.

        Returns:

        Example:

        ```python
        >>> from transformers import VitsTokenizer, VitsModel, set_seed
        >>> import torch

        >>> tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
        >>> model = VitsModel.from_pretrained("facebook/mms-tts-eng")

        >>> inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

        >>> set_seed(555)  # make deterministic

        >>> with torch.no_grad():
        ...     outputs = model(inputs["input_ids"])
        >>> outputs.waveform.shape
        torch.Size([1, 45824])
        ```
        """
        ...
    


