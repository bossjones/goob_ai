"""
This type stub file was generated by pyright.
"""

from collections import OrderedDict
from collections.abc import MutableMapping
from contextlib import contextmanager
from enum import Enum
from typing import Any, ContextManager, List, Tuple
from .import_utils import is_flax_available, is_torch_available

"""
Generic utilities
"""
if is_flax_available():
    ...
class cached_property(property):
    """
    Descriptor that mimics @property but caches output in member variable.

    From tensorflow_datasets

    Built-in in functools from Python 3.8.
    """
    def __get__(self, obj, objtype=...): # -> Self | Any:
        ...
    


def strtobool(val): # -> Literal[1, 0]:
    """Convert a string representation of truth to true (1) or false (0).

    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.
    Raises ValueError if 'val' is anything else.
    """
    ...

def infer_framework_from_repr(x): # -> Literal['pt', 'tf', 'jax', 'np', 'mlx'] | None:
    """
    Tries to guess the framework of an object `x` from its repr (brittle but will help in `is_tensor` to try the
    frameworks in a smart order, without the need to import the frameworks).
    """
    ...

def is_tensor(x): # -> bool:
    """
    Tests if `x` is a `torch.Tensor`, `tf.Tensor`, `jaxlib.xla_extension.DeviceArray`, `np.ndarray` or `mlx.array`
    in the order defined by `infer_framework_from_repr`
    """
    ...

def is_numpy_array(x): # -> bool:
    """
    Tests if `x` is a numpy array or not.
    """
    ...

def is_torch_tensor(x): # -> bool:
    """
    Tests if `x` is a torch tensor or not. Safe to call even if torch is not installed.
    """
    ...

def is_torch_device(x): # -> bool:
    """
    Tests if `x` is a torch device or not. Safe to call even if torch is not installed.
    """
    ...

def is_torch_dtype(x): # -> bool:
    """
    Tests if `x` is a torch dtype or not. Safe to call even if torch is not installed.
    """
    ...

def is_tf_tensor(x): # -> bool:
    """
    Tests if `x` is a tensorflow tensor or not. Safe to call even if tensorflow is not installed.
    """
    ...

def is_tf_symbolic_tensor(x): # -> Literal[False]:
    """
    Tests if `x` is a tensorflow symbolic tensor or not (ie. not eager). Safe to call even if tensorflow is not
    installed.
    """
    ...

def is_jax_tensor(x): # -> bool:
    """
    Tests if `x` is a Jax tensor or not. Safe to call even if jax is not installed.
    """
    ...

def is_mlx_array(x): # -> bool:
    """
    Tests if `x` is a mlx array or not. Safe to call even when mlx is not installed.
    """
    ...

def to_py_obj(obj): # -> dict[Any, Any] | list[Any] | Any:
    """
    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.
    """
    ...

def to_numpy(obj): # -> dict[Any, Any] | NDArray[Any]:
    """
    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a Numpy array.
    """
    ...

class ModelOutput(OrderedDict):
    """
    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a
    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular
    python dictionary.

    <Tip warning={true}>

    You can't unpack a `ModelOutput` directly. Use the [`~utils.ModelOutput.to_tuple`] method to convert it to a tuple
    before.

    </Tip>
    """
    def __init_subclass__(cls) -> None:
        """Register subclasses as pytree nodes.

        This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with
        `static_graph=True` with modules that output `ModelOutput` subclasses.
        """
        ...
    
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __post_init__(self): # -> None:
        """Check the ModelOutput dataclass.

        Only occurs if @dataclass decorator has been used.
        """
        ...
    
    def __delitem__(self, *args, **kwargs):
        ...
    
    def setdefault(self, *args, **kwargs):
        ...
    
    def pop(self, *args, **kwargs):
        ...
    
    def update(self, *args, **kwargs):
        ...
    
    def __getitem__(self, k): # -> Any:
        ...
    
    def __setattr__(self, name, value): # -> None:
        ...
    
    def __setitem__(self, key, value): # -> None:
        ...
    
    def __reduce__(self): # -> str | tuple[Any, ...] | tuple[str | Any, tuple[Any, ...], *tuple[str | Any, ...]]:
        ...
    
    def to_tuple(self) -> Tuple[Any]:
        """
        Convert self to a tuple containing all the attributes/keys that are not `None`.
        """
        ...
    


if is_torch_available():
    ...
class ExplicitEnum(str, Enum):
    """
    Enum with more explicit error message for missing values.
    """
    ...


class PaddingStrategy(ExplicitEnum):
    """
    Possible values for the `padding` argument in [`PreTrainedTokenizerBase.__call__`]. Useful for tab-completion in an
    IDE.
    """
    LONGEST = ...
    MAX_LENGTH = ...
    DO_NOT_PAD = ...


class TensorType(ExplicitEnum):
    """
    Possible values for the `return_tensors` argument in [`PreTrainedTokenizerBase.__call__`]. Useful for
    tab-completion in an IDE.
    """
    PYTORCH = ...
    TENSORFLOW = ...
    NUMPY = ...
    JAX = ...
    MLX = ...


class ContextManagers:
    """
    Wrapper for `contextlib.ExitStack` which enters a collection of context managers. Adaptation of `ContextManagers`
    in the `fastcore` library.
    """
    def __init__(self, context_managers: List[ContextManager]) -> None:
        ...
    
    def __enter__(self): # -> None:
        ...
    
    def __exit__(self, *args, **kwargs): # -> None:
        ...
    


def can_return_loss(model_class): # -> bool:
    """
    Check if a given model can return loss.

    Args:
        model_class (`type`): The class of the model.
    """
    ...

def find_labels(model_class): # -> list[str]:
    """
    Find the labels used by a given model.

    Args:
        model_class (`type`): The class of the model.
    """
    ...

def flatten_dict(d: MutableMapping, parent_key: str = ..., delimiter: str = ...): # -> dict[str | Any, Any]:
    """Flatten a nested dict into a single level dict."""
    ...

@contextmanager
def working_or_temp_dir(working_dir, use_temp_dir: bool = ...): # -> Generator[str | Any, Any, None]:
    ...

def transpose(array, axes=...): # -> NDArray[Any]:
    """
    Framework-agnostic version of `numpy.transpose` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def reshape(array, newshape): # -> NDArray[Any]:
    """
    Framework-agnostic version of `numpy.reshape` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def squeeze(array, axis=...):
    """
    Framework-agnostic version of `numpy.squeeze` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def expand_dims(array, axis): # -> NDArray[Any]:
    """
    Framework-agnostic version of `numpy.expand_dims` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def tensor_size(array): # -> int:
    """
    Framework-agnostic version of `numpy.size` that will work on torch/TensorFlow/Jax tensors as well as NumPy arrays.
    """
    ...

def add_model_info_to_auto_map(auto_map, repo_id):
    """
    Adds the information of the repo_id to a given auto map.
    """
    ...

def infer_framework(model_class): # -> Literal['tf', 'pt', 'flax']:
    """
    Infers the framework of a given model without using isinstance(), because we cannot guarantee that the relevant
    classes are imported or available.
    """
    ...

