"""
This type stub file was generated by pyright.
"""

import uuid
from typing import Any, Dict, List, Union
from ..utils import add_end_docstrings, is_tf_available, is_torch_available
from .base import Pipeline, build_pipeline_init_args

if is_tf_available():
    ...
if is_torch_available():
    ...
logger = ...
class Conversation:
    """
    Utility class containing a conversation and its history. This class is meant to be used as an input to the
    [`ConversationalPipeline`]. The conversation contains several utility functions to manage the addition of new user
    inputs and generated model responses.

    Arguments:
        messages (Union[str, List[Dict[str, str]]], *optional*):
            The initial messages to start the conversation, either a string, or a list of dicts containing "role" and
            "content" keys. If a string is passed, it is interpreted as a single message with the "user" role.
        conversation_id (`uuid.UUID`, *optional*):
            Unique identifier for the conversation. If not provided, a random UUID4 id will be assigned to the
            conversation.

    Usage:

    ```python
    conversation = Conversation("Going to the movies tonight - any suggestions?")
    conversation.add_message({"role": "assistant", "content": "The Big lebowski."})
    conversation.add_message({"role": "user", "content": "Is it good?"})
    ```"""
    def __init__(self, messages: Union[str, List[Dict[str, str]]] = ..., conversation_id: uuid.UUID = ..., **deprecated_kwargs) -> None:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def add_message(self, message: Dict[str, str]): # -> None:
        ...
    
    def add_user_input(self, text: str, overwrite: bool = ...): # -> None:
        """
        Add a user input to the conversation for the next round. This is a legacy method that assumes that inputs must
        alternate user/assistant/user/assistant, and so will not add multiple user messages in succession. We recommend
        just using `add_message` with role "user" instead.
        """
        ...
    
    def append_response(self, response: str): # -> None:
        """
        This is a legacy method. We recommend just using `add_message` with an appropriate role instead.
        """
        ...
    
    def mark_processed(self): # -> None:
        """
        This is a legacy method, as the Conversation no longer distinguishes between processed and unprocessed user
        input. We set a counter here to keep behaviour mostly backward-compatible, but in general you should just read
        the messages directly when writing new code.
        """
        ...
    
    def __iter__(self): # -> Generator[Dict[str, str], Any, None]:
        ...
    
    def __getitem__(self, item):
        ...
    
    def __setitem__(self, key, value): # -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __repr__(self): # -> str:
        """
        Generates a string representation of the conversation.

        Returns:
            `str`:

        Example:
            Conversation id: 7d15686b-dc94-49f2-9c4b-c9eac6a1f114 user: Going to the movies tonight - any suggestions?
            bot: The Big Lebowski
        """
        ...
    
    def iter_texts(self): # -> Generator[tuple[bool, str], Any, None]:
        ...
    
    @property
    def past_user_inputs(self): # -> list[Any] | list[str]:
        ...
    
    @property
    def generated_responses(self): # -> list[str]:
        ...
    
    @property
    def new_user_input(self): # -> str:
        ...
    


@add_end_docstrings(build_pipeline_init_args(has_tokenizer=True), r"""
        min_length_for_response (`int`, *optional*, defaults to 32):
            The minimum length (in number of tokens) for a response.""")
class ConversationalPipeline(Pipeline):
    """
    Multi-turn conversational pipeline.

    Example:

    ```python
    >>> from transformers import pipeline, Conversation
    # Any model with a chat template can be used in a ConversationalPipeline.

    >>> chatbot = pipeline(model="facebook/blenderbot-400M-distill")
    >>> # Conversation objects initialized with a string will treat it as a user message
    >>> conversation = Conversation("I'm looking for a movie - what's your favourite one?")
    >>> conversation = chatbot(conversation)
    >>> conversation.messages[-1]["content"]
    "I don't really have a favorite movie, but I do like action movies. What about you?"

    >>> conversation.add_message({"role": "user", "content": "That's interesting, why do you like action movies?"})
    >>> conversation = chatbot(conversation)
    >>> conversation.messages[-1]["content"]
    " I think it's just because they're so fast-paced and action-fantastic."
    ```

    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

    This conversational pipeline can currently be loaded from [`pipeline`] using the following task identifier:
    `"conversational"`.

    This pipeline can be used with any model that has a [chat
    template](https://huggingface.co/docs/transformers/chat_templating) set.
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __call__(self, conversations: Union[List[Dict], Conversation, List[Conversation]], num_workers=..., **kwargs): # -> Any | PipelineIterator | Generator[Any, Any, None] | Tensor | list[Any] | None:
        r"""
        Generate responses for the conversation(s) given as inputs.

        Args:
            conversations (a [`Conversation`] or a list of [`Conversation`]):
                Conversation to generate responses for. Inputs can also be passed as a list of dictionaries with `role`
                and `content` keys - in this case, they will be converted to `Conversation` objects automatically.
                Multiple conversations in either format may be passed as a list.
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):
                Whether or not to clean up the potential extra spaces in the text output.
            generate_kwargs:
                Additional keyword arguments to pass along to the generate method of the model (see the generate method
                corresponding to your framework [here](./model#generative-models)).

        Returns:
            [`Conversation`] or a list of [`Conversation`]: Conversation(s) with updated generated responses for those
            containing a new user input.
        """
        ...
    
    def preprocess(self, conversation: Conversation, min_length_for_response=...) -> Dict[str, Any]:
        ...
    
    def postprocess(self, model_outputs, clean_up_tokenization_spaces=...):
        ...
    


