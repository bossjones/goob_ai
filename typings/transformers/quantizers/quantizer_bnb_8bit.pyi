"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union
from .base import HfQuantizer
from ..modeling_utils import PreTrainedModel
from ..utils import is_torch_available

if TYPE_CHECKING:
    ...
if is_torch_available():
    ...
logger = ...
class Bnb8BitHfQuantizer(HfQuantizer):
    """
    8-bit quantization from bitsandbytes quantization method:
        before loading: converts transformer layers into Linear8bitLt during loading: load 16bit weight and pass to the
        layer object after: quantizes individual weights in Linear8bitLt into 8bit at fitst .cuda() call
    saving:
        from state dict, as usual; saves weights and 'SCB' component
    loading:
        need to locate SCB component and pass to the Linear8bitLt object
    """
    use_keep_in_fp32_modules = ...
    requires_parameters_quantization = ...
    requires_calibration = ...
    required_packages = ...
    def __init__(self, quantization_config, **kwargs) -> None:
        ...
    
    def validate_environment(self, *args, **kwargs): # -> None:
        ...
    
    def adjust_max_memory(self, max_memory: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]:
        ...
    
    def update_torch_dtype(self, torch_dtype: torch.dtype) -> torch.dtype:
        ...
    
    def update_device_map(self, device_map): # -> dict[str, int] | Dict[str, Any]:
        ...
    
    def adjust_target_dtype(self, target_dtype: torch.dtype) -> torch.dtype:
        ...
    
    def check_quantized_param(self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: Dict[str, Any], **kwargs): # -> bool:
        ...
    
    def create_quantized_param(self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, target_device: torch.device, state_dict: Dict[str, Any], unexpected_keys: Optional[List[str]] = ...): # -> None:
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        needs aux items from state dicts, if found - removes them from unexpected_keys
        """
        ...
    
    @property
    def is_serializable(self): # -> bool:
        ...
    
    @property
    def is_trainable(self) -> bool:
        ...
    


