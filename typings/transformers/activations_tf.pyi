"""
This type stub file was generated by pyright.
"""

import tensorflow as tf
from packaging.version import parse

def mish(x):
    ...

def gelu_fast(x):
    ...

def quick_gelu(x):
    ...

def gelu_10(x):
    """
    Clip the range of possible GeLU outputs between [-10, 10]. This is especially useful for quantization purpose, as
    it allows mapping 2 negatives values in the GeLU spectrum. For more information on this trick, please refer to
    https://arxiv.org/abs/2004.09602

    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when
    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see
    https://arxiv.org/abs/1606.08415 :param x: :return:
    """
    ...

def glu(x, axis=...):
    """
    Gated Linear Unit. Implementation as defined in the original paper (see https://arxiv.org/abs/1612.08083), where
    the input `x` is split in two halves across a dimension (`axis`), A and B, returning A * sigmoid(B).

    Args:
        `x`: float Tensor to perform activation
        `axis`: dimension across which `x` be split in half

    Returns:
        `x` with the GLU activation applied (with its size halved across the dimension `axis`).
    """
    ...

if parse(tf.version.VERSION) >= parse("2.4"):
    def approximate_gelu_wrap(x):
        ...
    
    gelu = ...
    gelu_new = ...
else:
    gelu = ...
    gelu_new = ...
ACT2FN = ...
def get_tf_activation(activation_string):
    ...

