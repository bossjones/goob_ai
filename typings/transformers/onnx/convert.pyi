"""
This type stub file was generated by pyright.
"""

from pathlib import Path
from typing import Iterable, List, TYPE_CHECKING, Tuple, Union
from packaging.version import Version
from ..utils import is_tf_available, is_torch_available
from .config import OnnxConfig
from ..modeling_utils import PreTrainedModel
from ..modeling_tf_utils import TFPreTrainedModel
from ..feature_extraction_utils import FeatureExtractionMixin
from ..processing_utils import ProcessorMixin
from ..tokenization_utils import PreTrainedTokenizer

if is_torch_available():
    ...
if is_tf_available():
    ...
if TYPE_CHECKING:
    ...
logger = ...
ORT_QUANTIZE_MINIMUM_VERSION = ...
def check_onnxruntime_requirements(minimum_version: Version): # -> None:
    """
    Check onnxruntime is installed and if the installed version match is recent enough

    Raises:
        ImportError: If onnxruntime is not installed or too old version is found
    """
    ...

def export_pytorch(preprocessor: Union[PreTrainedTokenizer, FeatureExtractionMixin, ProcessorMixin], model: PreTrainedModel, config: OnnxConfig, opset: int, output: Path, tokenizer: PreTrainedTokenizer = ..., device: str = ...) -> Tuple[List[str], List[str]]:
    """
    Export a PyTorch model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):
            The preprocessor used for encoding the data.
        model ([`PreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.
        device (`str`, *optional*, defaults to `cpu`):
            The device on which the ONNX model will be exported. Either `cpu` or `cuda`.

    Returns:
        `Tuple[List[str], List[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def export_tensorflow(preprocessor: Union[PreTrainedTokenizer, FeatureExtractionMixin], model: TFPreTrainedModel, config: OnnxConfig, opset: int, output: Path, tokenizer: PreTrainedTokenizer = ...) -> Tuple[List[str], List[str]]:
    """
    Export a TensorFlow model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`] or [`FeatureExtractionMixin`]):
            The preprocessor used for encoding the data.
        model ([`TFPreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.

    Returns:
        `Tuple[List[str], List[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def export(preprocessor: Union[PreTrainedTokenizer, FeatureExtractionMixin, ProcessorMixin], model: Union[PreTrainedModel, TFPreTrainedModel], config: OnnxConfig, opset: int, output: Path, tokenizer: PreTrainedTokenizer = ..., device: str = ...) -> Tuple[List[str], List[str]]:
    """
    Export a Pytorch or TensorFlow model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):
            The preprocessor used for encoding the data.
        model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.
        device (`str`, *optional*, defaults to `cpu`):
            The device on which the ONNX model will be exported. Either `cpu` or `cuda`. Only PyTorch is supported for
            export on CUDA devices.

    Returns:
        `Tuple[List[str], List[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def validate_model_outputs(config: OnnxConfig, preprocessor: Union[PreTrainedTokenizer, FeatureExtractionMixin, ProcessorMixin], reference_model: Union[PreTrainedModel, TFPreTrainedModel], onnx_model: Path, onnx_named_outputs: List[str], atol: float, tokenizer: PreTrainedTokenizer = ...): # -> None:
    ...

def ensure_model_and_config_inputs_match(model: Union[PreTrainedModel, TFPreTrainedModel], model_inputs: Iterable[str]) -> Tuple[bool, List[str]]:
    """

    :param model_inputs: :param config_inputs: :return:
    """
    ...

